---
title: "Power analysis for conceptual replication of S&P"
author: "Guillermo Montero-Melis"
date: '`r as.character(format(Sys.Date(), format="%d/%m/%Y"))`'
output:
  html_document:
    depth: 2
    number_sections: yes
    theme: default
    toc: yes
  pdf_document:
    toc: yes
---

```{r setup, include=FALSE}
library(knitr)
library(ggplot2)
# library(GGally)
library(plyr)
library(dplyr)
library(mvtnorm)
library(tidyr)
# library(lme4)
knitr::opts_chunk$set(echo = TRUE, fig.height = 3.5)
```


Introduction
============

We want to conduct a power analysis for a pilot study consisting in a conceptual
replication of Shebani & Pulvermüller (2013, Cortex), henceforth S&P.
In S&P, participants had to memorize groups of four words that were either arm
or leg related. They carried out a memory task in four different conditions:
no interference (control), arm interference, leg interference, and articulatory
interference.

Our pilot will simplify the original design and just try to replicate the 
most interesting effect reported in S&P: There
was an interaction of WordType (arm vs leg verb) and MovementType (arm vs
leg movements), leading to worse memory for arm than for leg verbs when
performing a complex drumming pattern with the hands/arms (arm movements),
and to worse memory for leg than for arm verbs when performing the drumming
pattern with the feet/legs (leg movements).


Challenges
----------

There are some basic difficulties for running a power analysis.
We would like to analyze the data using mixed logistic regression,
because the nature of the dependent variable is best modelled as binary at
the trial level (a quadruple is either remembered correctly or not).
However:

1. The original analyses report only by-subject ANOVAs computed either on the
mean number of errors per participant in each cell (Condition-by-Word Type)
or on z-transformed data. There is no information about item variability.
2. The number of trials in each experimental cell is not consistently reported
in S&P and the information provided is contradictory. This makes it difficult
to transform the reported results into proportions or log-odds.
3. The authors report several related ANOVAs, sometimes subsetting the number
of conditions and sometimes normalizing the data (i.e., z-transforming 
by-subject #errors)


What information *do* we have
---------------------------

- Variability in SDs (of by-subject means) is reported for the different cells
means; they also report effect sizes in terms of Cohen’s d, but I guess these
offer more indirect measures of by-subject variability.
- Apart from the original study, we do have estimates for base rate variability
between subjects, i.e. what would correspond to random by-subject intercepts.
These come from a pilot in which we only ran the control condition (no
interference). The information is not ideal, though, as it comes from L2 
speakers.



Reported results
=============================================

We start by running power analyses based on the reported results, taking them
at face value.

Here is how S&P report the relevant results (p.226):

> Most importantly, directly addressing the main hypothesis
motivating this study, a 2 x 2 analysis (Word Type x Moving
Body Part) revealed that when subjects engaged in performing
this rhythmic motor pattern with either their hands or feet,
errors in memory performance increased and a significant
interaction effect was found [F (1, 14) = 12.67, MSE = 20.9,
Cohen’s d = 1.25, p = .003] while no significant effect of condition
was found in this case (F < 1), documenting a differential
influence of movement type on word type performance.
Normalised z-transformed data confirmed this significant Word
Type x Moving Body Part Interaction [F (1, 14) = 25.49,
MSE= .92, Cohen’s d = 1.48, p < .0002; Fig. 1d]. For the z-transformed
data, in which the contribution of outliers and
between-subject variance is reduced, t-tests now also revealed
fully significant word category differences in both critical
movement interference conditions, for hand/arm movements
[more errors for arm words F (1,14)= 5.65, MSE = .67, Cohen’s
d = .73, p < .032] and foot/leg movements [more errors for leg
words, F (1, 14) = 11.26, MSE= .66, Cohen’s d = 1.0, p < .0047].

We will use the more conservative estimates, corresponding to the analyses
when data is *not* z-transformed.

We will thus use the values provided in Table 1 in S&B (p.226):

```{r}
sp_means <- expand.grid(WordType = c("arm-word", "leg-word"), Movement = c("arm", "leg"))
sp_means$M <- c(16.5, 14.6, 13.1, 16.9)
sp_means$SD <- c(8.39, 6.66, 6.23, 7.56)
```

```{r}
kable(sp_means)
```


Power analysis assuming normally-distributed data
=============================================

We may assume (as done by the authors) that the number of errors per participant
and condition follows a normal distribution. This is not ideal because, given
the variability estimates, this may lead to simulating *negative* numbers of
errors in certain cases. However, we will start like this.


Assuming uncorrelated observations within a subject
--------------------------------------------------

We start making the simplifying assumption that the cell means for each subject
are uncorrelated, i.e. the off-diagonal values in the variance-covariance
matrix are all zero.

This means that we assume that the number of errors a given participant makes,
say, when memorizing arm words in the leg-movement condition is uncorrelated
to the number of errors for the same words in the arm-movement condition.
This assumption should *decrease* power compared to assuming correlated data
within participants!

We run simulations of the following kind, where a run simulates a data set
from a distribution with the exact same parameters as those of the sample 
reported in the paper (note the cov matrix is 0 in all off-diagonals):

```{r, echo = TRUE}
rmvnorm(n = 15,  # Number of participants
        mean = sp_means$M,  # The mean of each cell as reported in Table 2
        sigma = diag(sp_means$SD ^ 2)  # vcov matrix obtained by squaring SD's (all off-diagonals = 0)
        )
```

Each row would represent the data from one participant. Each participant
contributes 4 data points (1 per column), corresponding to each of the four
cells. Let's rearrange the data in a more transparent way and put it all inside
a function:

```{r}
# function to simulate one data set
simulate_data <- function(my_n = 15, my_mean = sp_means$M, 
                     my_sigma = diag(sp_means$SD ^ 2)) {
  # simulate data
  rand_data <- rmvnorm(n = my_n, mean = my_mean, sigma = my_sigma)
  # By transposing the data it is stacked participant by participant
  rand_data <- t(rand_data)
  df <- data.frame(rand_data) %>% gather(value = nbErrors)  # to long format
  # Now give sensible column names (NB: data is stacked by participants)
  df$WordType <- c("arm-word", "leg-word")
  df$Movement <- rep(c("arm", "leg"), each = 2)
  df$Subject <- rep(1 : (nrow(df) / 4), each = 4)
  df$key <- NULL
  # Sensible column order
  df <- df %>% select(Subject : nbErrors)
  df
}
```


### A few example data sets

Simulate a few example data sets:

```{r}
set.seed(159753)
df_example <- rdply(6, simulate_data())
names(df_example)[1] <- "Simulation"
kable(head(df_example))
```



Plot the simulated data sets:

```{r}
ggplot(df_example, aes(x = Movement, y = nbErrors, colour = WordType)) +
  geom_boxplot() + facet_wrap(~ Simulation)
```

Plot means and 95% confidence intervals (assuming normal distribution)

```{r}
se <- function(x) { sd(x) / sqrt(length(x)) }
plotCI <- function(df) {
  pd <- position_dodge(0.2) # move them .1 to the left and right
  summ <- df %>% 
    group_by(Simulation, Movement, WordType) %>% 
    summarise(M = mean(nbErrors), SD = sd(nbErrors), SE = se(nbErrors))
  print(head(summ))  # print data summary
  p <- ggplot(summ, 
              aes(x = Movement, y = M, colour = WordType, shape = WordType,
                  group = WordType, ymax = M + 2 * SE, ymin = M - 2 * SE)) +
    geom_point(position = pd) +
    geom_errorbar(position = pd, width = .1) +
    geom_line(position = pd) +
    facet_wrap(~ Simulation)
  p
}
plotCI(df_example)
```

We can see that there is a lot of variability in the data, which leads to
qualitative differences in whether the interaction is visible or not and,
in general, to large confidence intervals.


### Estimate power


```{r}
# for functions from the plyr family, see
# http://nicercode.github.io/2014-02-13-UNSW/lessons/40-repeating/

# For simulating data from a mvnorm with a cov matrix computed from correlations, see
# https://stats.stackexchange.com/questions/164471/generating-a-simulated-dataset-from-a-correlation-matrix-with-means-and-standard
```




