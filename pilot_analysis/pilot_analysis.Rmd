---
title: "Analysis of pilot study"
author: '[Guillermo Montero-Melis](http://www.biling.su.se/montero_melis_guillermo)'
date: '`r as.character(format(Sys.Date(), format="%d/%m/%Y"))`'
output:
  html_document:
    depth: 2
    number_sections: yes
    theme: default
    toc: yes
  pdf_document:
    number_sections: yes
    toc: yes
  word_document:
    toc: yes
---

```{r setup, include=FALSE}
library(knitr)
library(ggplot2)
library(GGally)
library(dplyr)
library(tidyr)
library(lme4)
knitr::opts_chunk$set(echo = FALSE)
```

```{r}
# load convenience functions
source("../Rfunctions/load_or_fit_fnc.R")
```

Introduction and summary
========================


Memory task
===========

```{r cars}
memw <- read.csv("data_pilot_memory-task_wide.csv")
meml <- read.csv("data_pilot_memory-task_long.csv")
```

In this task, participants saw groups of four verbs flashed on the screen one after another.
After the fourth word, participants had to keep the words in memory for 6 seconds. After 6 seconds, they heard a beep, which provided the cue to repeat the four words they had just seen, in the exact same order.

We will call an **item** a set of four verbs (i.e., a quadruple). Items consist of either *arm-related verbs* (e.g., grab, dunk, lift, fold) or *leg-related verbs* (e.g., wander, step, stride, limp).

```{r include=FALSE}
table(unique(meml[, c("type", "verb")])$type)
```

Each participant carried out 4 blocks in which they had to memorize 28 items, 14 quadruples of arm-related, and 14 quadruples of leg-related verbs.
This means there were 56 verbs of each type.
What varied between the blocks was the time for which each word was shown: 
`r sort(unique(memw$word_duration))` ms.
The time between the offset of a verb and the onset of the next verb was held constant at 400 ms.

We will analyze the data in two ways:

1. By items or quadruples, counting an item as correct if all four verbs were remembered in the correct order
2. By verbs, by looking whether each individual verb in a quadruple was remembered.


## By-subject analysis at the level of item (quadruples)

### Nature of the data (wide format)

Here is an example of the data set when analyzed by items:

```{r}
kable(head(memw[, c(3:5, 7:8, 10:13, 18:19)]))
```

A few columns from the data file are omitted here, but these are the most important ones.
Glossing over the first row:

- It corresponds to `participant`'s 900 first target item (`trial` = 1) in the first block (`block` = 1)
- In that block each word was shown for 200 ms (`word_duration` = 200).
- It was an item of arm-related verbs (`type` = arm) and the four words shown were *grab*, *dunk*, *lift* and *file* (`word1`--`word4`).
- The participant did not make any error (`error` is blank) and thus achieved a `score` of 1.

The `score` column rates items (quadruples) as either correct (=1) if all verbs are remembered in the right order, or as wrong (=0) if some error is made by the participant.
The `error`  column contains abbreviations for the types of error coded for (following Shebani & PulvermÃ¼ller, 2013):

- *Omission* (O): At least one target word is omitted
- *Replacement* (R): One target word is replaced by a non-target word
- *Shift/transposition* (S): Two target words are shuffled
- *Addition* (A): It is the case where none of the previous errors have occurred (all 4 target verbs were retrieved in the right order), but the participant added at least one more verb that is not a target. NB: This was not in the original S&P.


Note that some observations were removed because participants did something
they were not supposed to (e.g., whispering the words during the memory phase)
or because the answers were not perceivable.
This leads to some participants having missing observations for a given word
presentation rate.
Observations were always removed at the item (quadruple level), so that if one
word was unclear, then the whole item (all four verbs for that trial) was
deleted.


```{r}
with(memw, table(word_duration, participant))
```

The most extreme case is participant 902 under the word presentation rate of
300 ms, for whom `r 28-19` observations are missing 
(`r round(100 * (28 - 19) / 28)`%).
For now, we leave all participants in, as there is no reason to assume that this
resulted in a special bias.


### Error rates by word duration

The goal was to find a setting of word duration that would leave participants at approximately 30% error rates in the control condition tested here (see S&P, p.X).

The following figure shows mean error rates (red dots) and confidence intervals of by-subject means (red lines) for each word duration setting (x-axis). The smaller grayish dots show individual by-subject means.


```{r}
ppt_mean <- memw %>% 
  group_by(participant, block, word_duration) %>%
  summarise(error_rate = 1 - mean(score))
ylims <- c(0,1)
ggplot(ppt_mean, aes(x = word_duration, y = error_rate)) +
  stat_summary(fun.y = "mean", colour = "red", size = 1, geom = "point") +
  stat_summary(fun.data = "mean_cl_boot", colour = "red", size = 2) +
  geom_jitter(width = 20, height = 0, alpha = .4) +
  ylab("Error rate") + xlab("word duration (in ms)") +
  ylim(ylims)
```

We see that L2 speakers on average had error rates at or above 50%.
Even when each word was shown for 400 ms, error rates were around 50%.
Overall, word duration did not seem to have a very strong effect.
We will follow up with more detailed analyses below.

We can also break down observations by the type of verbs (arm- vs leg-related):

```{r}
ppt_mean_ty <- memw %>% 
  group_by(participant, block, word_duration, type) %>%
  summarise(error_rate = 1 - mean(score))

ggplot(ppt_mean_ty, aes(x = word_duration, y = error_rate, colour = type)) +
  stat_summary(fun.y = "mean", size = 1, geom = "point") +
  stat_summary(fun.data = "mean_cl_boot", size = 2, position = position_dodge(width = 20)) +
  geom_jitter(width = 20, height = 0, alpha = .4) +
  ylab("Error rate") + xlab("word duration (in ms)") +
  ylim(ylims)
```

It seems like both types of verbs were of comparable difficulty.


### Error rates by block

Perhaps more important than the word duration was the actual block number, because error rates could drop as participants proceded into the experiment and already had encountered the verbs (since the same verbs were shown in all blocks).

```{r}
ggplot(ppt_mean, aes(x = block, y = error_rate)) +
  stat_summary(fun.y = "mean", colour = "red", size = 1, geom = "point") +
  stat_summary(fun.data = "mean_cl_boot", colour = "red", size = 2) +
  geom_jitter(width = .2, height = 0, alpha = .4) +
  ylab("Error rate") +
  ylim(ylims)
```

Indeed there seems to be a drop of error rates from the 1st to the 2nd block, perhaps because participants needed a block to get familiarized with the task.
Error rates did not drop much in subsequent blocks after the 2nd.

Again, the patterns were similar if we break down the verbs by type (arm- vs leg-related), as below:

```{r}
ggplot(ppt_mean_ty, aes(x = block, y = error_rate, colour = type)) +
  stat_summary(fun.y = "mean", size = 1, geom = "point") +
  stat_summary(fun.data = "mean_cl_boot", size = 2, position = position_dodge(width = .5)) +
  geom_jitter(width = .2, height = 0, alpha = .4) +
  ylab("Error rate") + 
  ylim(ylims)
```


### Types of errors

What were the most frequent error types?

```{r}
error_types <- memw %>%
  group_by(error, error_expl) %>%
  count() %>%
  mutate(percentage = round(100 * n / nrow(memw), 1))
levels(error_types$error_expl)[levels(error_types$error_expl) == ""] <- "No error"
error_types <- error_types %>% rename(error_explicit = error_expl)
error_types <- error_types[order(- error_types$n), ]
kable(error_types[order(- error_types$n), ])
```

The table shows that most of the errors are either replacement or omission
errors (or both). These two categories *alone* (i.e., not combined with any 
other type of error) account for 
**`r round(100 * sum(error_types$n[2:4]) / sum(error_types$n[2:nrow(error_types)]))`%**
of all the items/quadruples that were not remembered correctly.


### Participant variability

The plots in the previous section suggest substantial individual variability.
Let us look at this by plotting each participant's mean error rate (averaged across blocks).

```{r}
ppt_mean_simp <- memw %>% 
  group_by(participant) %>%
  summarise(error_rate = 1 - mean(score))
ppt_mean_simp$participant <- factor(ppt_mean_simp$participant, levels = ppt_mean_simp$participant[order(ppt_mean_simp$error_rate)])
ggplot(ppt_mean_simp, aes(x = participant, y = error_rate)) +
  geom_bar(stat = "identity") +
  ylab("Error rate") +
  ylim(ylims)
```

Variability is quite large (range: `r round(range(ppt_mean_simp$error_rate), 2)`).


#### Within participant correlation between verb types (arm/leg)

We can ask if error rates for the two types of verbs were correlated (by participants)?

```{r}
ppt_mean_simp_type <- memw %>% 
  group_by(participant, type) %>%
  summarise(error_rate = 1 - mean(score)) %>%
  spread(type, error_rate)  # reshape to wide format for correlation plot
ggplot(ppt_mean_simp_type, aes(x = arm, y = leg)) +
  geom_point() +
  xlab("Error rate for arm items") + ylab("Error rate for leg items") + 
  geom_smooth(method = "lm")
```

The correlation is indeed high (*r* = `r round(with(ppt_mean_simp_type, cor(arm, leg)), 2)`), again suggesting that there were no marked differences between the number of errors a participant made for either type of items:

```{r}
with(ppt_mean_simp_type, cor.test(arm, leg))
```


#### Within participant correlation between different word durations

We can use a scatterplot matrix to check whether the by-participant correlation between different word durations was high:

```{r}
ppt_mean_simp_durat <- memw %>% 
  group_by(participant, word_duration) %>%
  summarise(error_rate = 1 - mean(score)) %>%
  spread(word_duration, error_rate)  # reshape to wide format for correlation plot
```

```{r, eval = FALSE}
ggpairs(ppt_mean_simp_durat[, 2:5], lower = list(continuous = "smooth"))
```


Correlations are high, suggesting that, generally, a participant's error rate for a particular word duration was predictive of their error rates with another word duration.


#### Within participant correlation between blocks

We can use the same scatterplot matrix approach to check whether the by-participant correlation between blocks also was high:

```{r}
ppt_mean_simp_block <- memw %>% 
  group_by(participant, block) %>%
  summarise(error_rate = 1 - mean(score)) %>%
  spread(block, error_rate)  # reshape to wide format for correlation plot
```

```{r, eval = FALSE}
ggpairs(ppt_mean_simp_block[, 2:5], lower = list(continuous = "smooth"))
```


And indeed it was -- even higher than the correlation by word-duration!



### Logistic mixed model analysis

To analyze the data statistically, we fit a logistic mixed model.

The dependent variable is binary (`score`).
The predictor variables are:

- `word_duration` dummy coded (`r levels(factor(memw$word_duration))` ms)
- `block` dummy coded (1, 2, 3, 4)
- `type` dummy coded (`r levels(factor(memw$type))`)

We can only have participants as random effects, including random intercepts
as well as (at most) random slopes for all three predictors.
As we will see below, random slopes lead to convergence failures.
No random effects for items can be fit, since the verbs that made up
the quadruples were randomly chosen each time.


```{r, echo = TRUE}
# convert variables to factors:
memw$f_word_duration <- factor(memw$word_duration)
memw$f_block <- factor(memw$block)

# contrast coding for verb type
contrasts(memw$type) <- contr.sum(2) / 2
colnames(contrasts(memw$type)) <- "arm_vs_leg"
# use forward difference coding, see
# https://stats.idre.ucla.edu/r/library/r-library-contrast-coding-systems-for-categorical-variables/
# In this coding system, the mean of the dependent variable for one level of
# the categorical variable is compared to the mean of the dependent variable
# for the next (adjacent) level.

#forward difference for factor variable with 4 levels
my.forward.diff <- matrix(c( 3/4, -1/4, -1/4,
                            -1/4,  1/2,  1/2, 
                            -1/2, -1/2,  1/4,
                             1/4,  1/4, -3/4), 
                          ncol = 3)
# word presentation rate
contrasts(memw$f_word_duration) <- my.forward.diff
colnames(contrasts(memw$f_word_duration)) <- c("100_vs_200",
                                               "300_vs_200",
                                               "400_vs_300")
# block
contrasts(memw$f_block) <- my.forward.diff
colnames(contrasts(memw$f_block)) <- c("1_vs_2", "3_vs_2", "4_vs_3")

# History of fitted models:

# # The maximal model fails to converge:
# glmm_quadr <- glmer(score ~ f_word_duration + f_block + type +
#                       (1 + f_word_duration + f_block + type | participant),
#                     data = memw,
#                     family = 'binomial')
# # So does a model removing the (nuisance) variable block
# glmm_quadr <- glmer(score ~ f_word_duration + f_block + type +
#                       (1 + f_word_duration + type | participant),
#                     data = memw,
#                     family = 'binomial')
# #  Even if we just leave f_word_duration as a random slope, it doesn't converge.
# glmm_quadr <- glmer(score ~ f_word_duration + f_block + type +
#                       (1 + f_word_duration | participant),
#                     data = memw,
#                     family = 'binomial')

# So we go for a random by-subject intercept only:
glmm_quadr.expr <- "glmer(score ~ f_word_duration + f_block + type +
                      (1 | participant),
                    data = memw,
                    family = 'binomial')"
# load if saved or fit it
load_or_fit("glmm_quadr", glmm_quadr.expr)
summary(glmm_quadr)
```


Compare to dummy coding of `word_duration` and `block`:

```{r}
# Same model using dummy coding
glmm_quadr_dummy.expr <- "glmer(score ~ factor(word_duration) + factor(block) + type +
                            (1 | participant),
                          data = memw, family = 'binomial')"
# load if saved or fit it
load_or_fit("glmm_quadr_dummy", glmm_quadr_dummy.expr)

summary(glmm_quadr_dummy)
```



## By-subject analysis at the level of individual verbs (rather than items/quadruples)

Instead of just looking at whole items (quadruples), we now consider individual words.


### Nature of the data (long format)

To look at the data by verb rather than by item, we need the data in long format.

```{r}
# Show data in long format
kable(head(meml))
```

Notice now each row corresponds to a single verb.
For instance, the first 4 rows of the table above show the 4 verbs from the 
first item of the first block, which were all remembered correctly by 
participant 900.
This participant did not, however, remember the 2nd word (`wordIntTrial` = 2, namely *clutch*)
in trial 2.


### Error rates by word duration

We begin with a plot equivalent to the one we did for whole items, but now we show the error rates when we consider whether individual verbs were remembered.
Note that this means that an item for which 3 of the 4 verbs were remembered would yield a 25% error rate (whereas it led to a 100% error rate under the more severe coding by items).

```{r}
ppt_mean_l <- meml %>% 
  group_by(participant, block, word_duration) %>%
  summarise(error_rate = 1 - mean(correct))
ggplot(ppt_mean_l, aes(x = word_duration, y = error_rate)) +
  stat_summary(fun.y = "mean", colour = "red", size = 1, geom = "point") +
  stat_summary(fun.data = "mean_cl_boot", colour = "red", size = 2) +
  geom_jitter(width = 20, height = 0, alpha = .4) +
  ylab("Error rate") + xlab("word duration (in ms)") +
  ylim(ylims)
```

Under this treatment, error rates diminish drastically!
We now lie at a mean error rate below 25% and no participant has a mean error rate above 50%.
The effect of word duration becomes less clear.
But let us rescale the y-axis so it is used for the range of the data
rather than forcing the y-scale to lie between 0 and 1. 
This yields better resolution to see variability.


```{r}
ylims2 <- c(0, .55)
ggplot(ppt_mean_l, aes(x = word_duration, y = error_rate)) +
  stat_summary(fun.y = "mean", colour = "red", size = 1, geom = "point") +
  stat_summary(fun.data = "mean_cl_boot", colour = "red", size = 2) +
  geom_jitter(width = 20, height = 0, alpha = .4) +
  ylab("Error rate") + xlab("word duration (in ms)") +
  ylim(ylims2)
```


We can also break down observations by the type of verbs (arm- vs leg-related).

```{r}
ppt_mean_l_ty <- meml %>% 
  group_by(participant, block, word_duration, type) %>%
  summarise(error_rate = 1 - mean(correct))
ggplot(ppt_mean_l_ty, aes(x = word_duration, y = error_rate, colour = type)) +
  stat_summary(fun.y = "mean", size = 1, geom = "point") +
  stat_summary(fun.data = "mean_cl_boot", size = 2, position = position_dodge(width = 20)) +
  geom_jitter(width = 20, height = 0, alpha = .4) +
  ylab("Error rate") + xlab("word duration (in ms)") +
  ylim(ylims2)
```

It seems like, under this analysis as well, both types of verbs were of comparable difficulty.


### Error rates by block

Was there a difference between the different blocks?

```{r}
ggplot(ppt_mean_l, aes(x = block, y = error_rate)) +
  stat_summary(fun.y = "mean", colour = "red", size = 1, geom = "point") +
  stat_summary(fun.data = "mean_cl_boot", colour = "red", size = 2) +
  geom_jitter(width = .2, height = 0, alpha = .4) +
  ylab("Error rate") +
  ylim(ylims2)
```

We see the same picture as before:
Error rates drop from the 1st to the 2nd block (from slightly under 30% to slightly over 20%), but then remain stable throughout
the rest of the blocks.

This does not differ between verb types, as shown below:

```{r}
ggplot(ppt_mean_l_ty, aes(x = block, y = error_rate, colour = type)) +
  stat_summary(fun.y = "mean", size = 1, geom = "point") +
  stat_summary(fun.data = "mean_cl_boot", size = 2, position = position_dodge(width = .4)) +
  geom_jitter(width = .2, height = 0, alpha = .4) +
  ylab("Error rate") +
  ylim(ylims2)
```


### Effect of verb position within an item

Was the position in which a verb occured in the item (word 1--4) predictive of how well it was remembered?
This seems likely, as we know that in lists the first and last items are better remembered than those in the middle.

```{r}
ppt_mean_l_wordInTrial <- meml %>% 
  group_by(participant, wordInTrial) %>%
  summarise(error_rate = 1 - mean(correct))

ggplot(ppt_mean_l_wordInTrial, aes(x = wordInTrial, y = error_rate)) +
  stat_summary(fun.y = "mean", size = 1, geom = "point") +
  stat_summary(fun.data = "mean_cl_boot", size = 2) +
  geom_jitter(height = 0, width = .2, alpha = .4) +
  ylab("Error rate") + xlab("Verb position") +
  ylim(ylims2)
```

The position of the verb indeed seems to matter:
the first verb in a quadruple tended to be better remembered than any of the
following. 
But performance was not better for the last one than for the middle ones,
as one might have expected.


We can further investigate whether this effect changed across blocks:

```{r}
ppt_mean_l_wordInTrial_block <- meml %>% 
  group_by(participant, wordInTrial, block) %>%
  summarise(error_rate = 1 - mean(correct))
ggplot(ppt_mean_l_wordInTrial_block, aes(x = wordInTrial, y = error_rate)) +
  stat_summary(fun.y = "mean", size = 1, geom = "point") +
  stat_summary(fun.data = "mean_cl_boot", size = 2) +
  geom_jitter(height = 0, width = .2, alpha = .4) +
  facet_grid(. ~ block) +
  ylab("Error rate") + xlab("Verb position")  +
  ylim(0, .6)
```

The effect does not seem to change much between blocks.
(There is only an overall shift downwards from block 1 to block 2, as we saw
before, but the relative position of the means depending on word position is
similar).



### Participant variability

Let us consider if the picture for participant variability changes when looking
at it by individual verbs rather than quadruples.
We plot each participant's mean error rate averaged across blocks:

```{r}
ppt_mean_l_simp <- meml %>% 
  group_by(participant) %>%
  summarise(error_rate = 1 - mean(correct))
ppt_mean_l_simp$participant <- factor(ppt_mean_l_simp$participant, levels = ppt_mean_l_simp$participant[order(ppt_mean_l_simp$error_rate)])
ggplot(ppt_mean_l_simp, aes(x = participant, y = error_rate)) +
  geom_bar(stat = "identity") +
  ylab("Error rate") +
  ylim(ylims)
```

Participant variability is much reduced when error rates are computed by 
individual verbs. The range is: `r round(range(ppt_mean_l_simp$error_rate), 2)`
(whereas it was `r round(range(ppt_mean_simp$error_rate), 2)` when computed by items).

We can see if the two ways of measuring error rates correlate well within 
participants:

```{r}
# join the two tables
ppt_mean_simp$participant <- as.character(ppt_mean_simp$participant)
ppt_mean_l_simp$participant <- as.character(ppt_mean_l_simp$participant)
error_rates <- left_join(ppt_mean_simp, ppt_mean_l_simp %>%
                           rename(error_rate_verb = error_rate))
# plot correlation
ggplot(error_rates, aes(x = error_rate_verb, y = error_rate)) +
  geom_point() +
  xlab("Error rate for individual verbs") + ylab("Error rate for items (quadruples)") + 
  geom_smooth(method = "lm")
```

The correlation is very high 
(*r* = `r round(with(error_rates, cor(error_rate_verb,error_rate)), 2)`),
suggesting that computing errors in one way or another does not order participants
differently.
This is good news, as we may then pretty safely use the error rate by verbs,
which is a more sensitive measure.



## Verb analysis

We can use participant performance on the memory task to learn something about
the difficulty of the verbs.
To do that, we look at whether individual verbs (not items) were remembered 
correctly.

First, here is a list of the verbs we used in this pilot:

- **Arm-related verbs**: `r sort(unique(meml[meml$type == "arm", "verb"]))`
- **Leg-related verbs**: `r sort(unique(meml[meml$type == "leg", "verb"]))`

Note that some observations were removed because participants did something
they were not supposed to (e.g., whispering the words during the memory phase)
or because the answers were not perceivable.
This leads to some verbs having 1 or 2 missing observations for a given word
presentation rate.

```{r}
missing_obs_per_verb <- meml %>% 
  group_by(verb, word_duration) %>%
  summarise(missing_observations = 17 - n())
missing_obs_per_verb[missing_obs_per_verb$missing_observations != 0, 2:3] %>%
  count(word_duration, missing_observations) %>%
  kable
```

The table above should be read as follows:

- Row 1: There are 8 verbs for which there is one missing observation (i.e. there is 
data for 16 out of 17 participants) for word_duration 100 ms.
- Row 2: There are 12 verbs for which there is one missing observation (i.e. there is 
data for 16 out of 17 participants) for word_duration 200 ms
- etc.


### Item variability

The average error rate of an item is the mean number of participants who 
remembered it (averaged across blocks):

```{r, fig.width=8}
item_mean_simp <- meml %>% 
  group_by(verb, type) %>%
  summarise(error_rate = 1 - mean(correct))
item_mean_simp$verb <- factor(item_mean_simp$verb, levels = item_mean_simp$verb[order(item_mean_simp$error_rate)])
ggplot(item_mean_simp, aes(x = verb, y = error_rate, fill = type)) +
  geom_bar(stat = "identity") +
  ylab("Error rate") +
  theme(axis.text.x = element_text(angle = 90, hjust = 1, size = 8)) +
  ylim(ylims2)
```

Variability is by far not as large as for participants
(range: `r round(range(item_mean_simp$error_rate), 2)`).
Another nice result this plot confirms is that there does not seem to a marked
difference in difficulty between the two types of verbs:
red and blue bars are spread out quite evenly along the whole range.

We can have a look at the easiest and hardest verbs to remember (of each type):

```{r}
my_order <- function(df = item_mean_simp, mytype = NULL, nb = 5) {
  d <- df[df$type == mytype, ]
  d <- d[order(d$error_rate), ]
  d$error_rate <- round(d$error_rate, 2)
  print(paste("The ", nb, " easiest ", mytype, "-verbs:", sep = ""))
  print(kable(head(d, nb)))
  cat("\n\n\n")
  print(paste("The ", nb, " hardest ", mytype, "-verbs:", sep = ""))
  print(kable(tail(d, nb)))
}
my_order(mytype = "arm")
my_order(mytype = "leg")
```


### By-item correlation of error rates between different word presentation rates

We use a scatterplot matrix to check whether the by-item correlations of error
rates under different word durations were high:

```{r}
item_mean_simp_durat_wide <- meml %>% 
  group_by(verb, type, word_duration) %>%
  summarise(error_rate = 1 - mean(correct)) %>%
  spread(word_duration, error_rate)  # reshape to wide format for correlation plot
```

```{r, eval = FALSE}
# ggpairs distinguishing between verb types
# (for setting binwidth in histograms, see
# https://github.com/ggobi/ggally/issues/184 and vignette
# http://ggobi.github.io/ggally/#function_wrapping )
ggpairs(item_mean_simp_durat_wide[, 2:6],
        mapping = ggplot2::aes(colour = type),
        lower = list(continuous = "smooth", 
                     combo=wrap("facethist", binwidth = 0.1)))
# # Or a simpler version that does not distinguish between verb types
# ggpairs(item_mean_simp_durat_wide[, 3:6], lower = list(continuous = "smooth"))
```


Not sure how surprising this is, but the correlations of by-item means are
much lower than those for by-subject means.
In other words, the fact that a given verb was hard to remember for one 
presentation rate does not imply that it was equally hard at another rate.


## Conclusion from memory task

A

- The analysis based on rating whole items (quadruples) as either correct or
incorrect resulted in a high error rate (around 50%) at all verb presentation
rates.
- This rate is higher than we were aiming for based on the original study S&P, for
whom error rates in the control condition (with 100 ms verb presentation rates)
lay at around 30%.
- Manipulating the presentation rate led at best to a marginally better
performance for a 400 ms presentation rate.

- No differences by verb type


- word_duration: not much effect
- individual items: ... + we're going to follow it up now in the analysis of the following two tasks


- Omission and replacement the most frequent errors


Verb norming task: arm vs leg bias
==================================

The data from the verb norming task look like this:

```{r}
bias_l <- read.csv("data_verb-bias.csv", fileEncoding = "UTF-8")
kable(head(bias_l[, 3:8]))
```

Each participant rated each of the 112 verbs on a arm-relatedness and
a leg-relatedness scale from 1 to 7.

Unfortunately, there was a mismatch in the verbs chosen for the memory and the
norming task, so that *trek* (a leg verb used in the memory task) was 
mistakenly replaced by *pluck* (an arm verb) in the norming task.
This leads to a slight imbalance in the types of verbs used for norming:

```{r}
table(unique(bias_l[, c("verb", "type")])$type)
```


## Verb bias

We can compute a single measure of *arm-minus-leg bias* per verb
by simply substracting the leg rating from the verb rating,
obtaining:

```{r}
bias <- bias_l %>%
  group_by(verb, type, rated_category) %>%
  summarise(rating = mean(rating)) %>%
  spread(rated_category, rating) %>%  # reshape to wide format for correlation plot
  mutate(bias = Arm - Leg)
kable(head(bias))
```

- A positive bias score (bias $> 0$) indicates bias towards arm-relatedness (max $6$)
- A negative bias score (bias $< 0$) indicates bias towards leg-relatedness (min $-6$)
- A bias score $= 0$ indicates absence of either bias

```{r}
# order factor levels of verbs depending on their bias (for plotting)
ordered_verbs_bias <- as.character(pull(bias[order(bias$bias), ], verb))
bias$verb <- factor(bias$verb, levels = ordered_verbs_bias)
```


```{r, fig.height = 8, fig.width = 8}
ggplot(bias, aes(x = verb, y = bias, colour = type)) +
  geom_point() +
  theme(axis.text.x = element_text(size = 6)) +
  coord_flip()
```

The figure shows that the bias was generally stronger for the verbs we had
categorized as arm verbs than for those we thought of as leg verbs.

This can be shown even clearer if we just consider the size of the bias in
the expected direction, so a negative bias now constitutes a bias in the 
"wrong" direction (e.g., an arm bias for a supposedly leg-related verb).
The following plot shows this:

```{r}
# We add a bias column that changes the sign for leg verbs, so that bias
# is expressed with respect to the expected direction of the bias for a verb
bias$bias_absol <- bias$bias
bias[bias$type == "leg", "bias_absol"] <- - bias[bias$type == "leg", "bias"]
ggplot(bias, aes(x = type, y = bias_absol, colour = type)) + 
  geom_boxplot() +
  ylab("Size of bias in predicted direction")
```

Only one verb had opposite bias to what was expected:

```{r}
kable(bias[bias$bias_absol < 0, ])
```

To scuff means to "Scrape or brush the surface of (a shoe or other object) 
against something" (OED).
But participants seemed not to know the verb very well and assume it had some
arm-related meaning.


## Cut-off at bias = 3

Somewhat arbitrarily, we could say that a bias of 3 in the expected direction
constitutes an acceptable cut-off point to select a verb for the study.
There are 
`r sum(with(bias, bias_absol >= 3 & type == "arm"))`
arm verbs (i.e., all but one) that satisfy this condition,
but only
`r sum(with(bias, bias_absol >= 3 & type == "leg"))`
leg verbs (out of
`r sum(with(bias, type == "leg"))`) that qualify.

Here are the leg verbs that have a clear bias:

```{r}
ggplot(bias[with(bias, bias_absol >= 3 & type == "leg"), ],
       aes(x = verb, y = bias)) +
  geom_point() +
  theme(axis.text.x = element_text(size = 6)) +
  coord_flip() +
  ggtitle("Leg verbs with strongest leg bias")
```


## Spread of ratings

Above we have only considered the mean rating for each verb (averaged across
participants).
But how much spread was there in the ratings from individual participants?

```{r}
bias_spread <- bias_l %>%
  select(participant, verb:rating) %>%
  spread(rated_category, rating) %>%
  mutate(bias = Arm - Leg)
# order verbs according to bias
bias_spread$verb <- factor(bias_spread$verb, levels = ordered_verbs_bias)
```


```{r, fig.height = 9}
ggplot(bias_spread, aes(x = verb, y = bias, colour = type)) +
  stat_summary(fun.y = "mean", geom = "point") +
  stat_summary(fun.data = "mean_cl_boot") +
  geom_point(alpha = .4) +
  ylab("Verb bias") + 
  coord_flip()
```

What this figure shows is that the average bias for a verb can still hide some
important individual differences.
For instance, the leg verb *kneel* had a fairly strong leg bias of
`r round(bias[bias$verb == "kneel", ]$bias, 1)`
and still there was one participant who rated it as completely arm-biased
(bias = 6).

Some of these data points might simply be due to participants getting the 
scales mixed up, but probably not all of these data points came about in this
way:
Similar observations hold for the supposedly leg verbs *tread* and *bounce*.
For the latter, one could imagine how a participant made associations with 
*bouncing a ball*, in which case the verb *to bounce* naturally becomes 
arm-related).

Finally, note also the many ratings of verbs as having no bias (verb bias = 0).
These may be the result of participants not knowing a verb meaning,
a matter to which we now turn.



Verb comprehension task
======================

```{r}

```

