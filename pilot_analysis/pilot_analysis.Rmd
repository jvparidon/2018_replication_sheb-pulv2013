---
title: "Analysis of pilot study"
author: '[Guillermo Montero-Melis](http://www.biling.su.se/montero_melis_guillermo)'
date: '`r as.character(format(Sys.Date(), format="%d/%m/%Y"))`'
output:
  pdf_document:
    number_sections: yes
    toc: yes
  html_document:
    depth: 2
    number_sections: yes
    theme: default
    toc: yes
  word_document:
    toc: yes
---

```{r setup, include=FALSE}
library(knitr)
library(ggplot2)
library(GGally)
library(dplyr)
library(tidyr)
library(lme4)
knitr::opts_chunk$set(echo = FALSE, fig.height = 3.5)
```

```{r}
# load convenience functions
source("../Rfunctions/load_or_fit_fnc.R")
```

Introduction and summary
========================


Memory task
===========

```{r cars}
memw <- read.csv("data_pilot_memory-task_wide.csv")
meml <- read.csv("data_pilot_memory-task_long.csv")
```

In this task, participants saw groups of four verbs flashed on the screen one after another.
After the fourth word, participants had to keep the words in memory for 6 seconds. After 6 seconds, they heard a beep, which provided the cue to repeat the four words they had just seen, in the exact same order.

We will call an **item** a set of four verbs (i.e., a quadruple). Items consist of either *arm-related verbs* (e.g., grab, dunk, lift, fold) or *leg-related verbs* (e.g., wander, step, stride, limp).

```{r include=FALSE}
table(unique(meml[, c("type", "verb")])$type)
```

Each participant carried out 4 blocks in which they had to memorize 28 items, 14 quadruples of arm-related, and 14 quadruples of leg-related verbs.
This means there were 56 verbs of each type.
What varied between the blocks was the time for which each word was shown: 
`r sort(unique(memw$word_duration))` ms.
The time between the offset of a verb and the onset of the next verb was held constant at 400 ms.

We will analyze the data in three ways:

1. Looking at by-subject averages when we consider whole items (or quadruples), i.e. counting an item as correct only if all four verbs were remembered in the correct order;
2. Looking at by-subject averages computed for particular verbs, that is, by looking whether each individual verb in a quadruple was remembered or not.
3. Considering item variability, i.e. as a by-item analysis.


## By-subject analysis at the level of *items* (quadruples)

### Nature of the data (wide format)

Here is an example of the data set when analyzed by items:

```{r}
kable(head(memw[, c(3:5, 7:8, 10:13, 18:19)]))
```

A few columns from the data file are omitted here, but these are the most important ones.
Glossing over the first row:

- It corresponds to `participant`'s 900 first target item (`trial` = 1) in the first block (`block` = 1)
- In that block each word was shown for 200 ms (`word_duration` = 200).
- It was an item of arm-related verbs (`type` = arm) and the four words shown were *grab*, *dunk*, *lift* and *file* (`word1`--`word4`).
- The participant did not make any error (`error` is blank) and thus achieved a `score` of 1.

The `score` column rates items (quadruples) as either correct (=1) if all verbs are remembered in the right order, or as wrong (=0) if some error is made by the participant.
The `error`  column contains abbreviations for the types of error coded for (following Shebani & PulvermÃ¼ller, 2013):

- *Omission* (O): At least one target word is omitted
- *Replacement* (R): One target word is replaced by a non-target word
- *Shift/transposition* (S): Two target words are shuffled
- *Addition* (A): It is the case where none of the previous errors have occurred (all 4 target verbs were retrieved in the right order), but the participant added at least one more verb that is not a target. NB: This was not in the original S&P.


Note that some observations were removed because participants did something
they were not supposed to (e.g., whispering the words during the memory phase)
or because the answers were unintelligible.
This means some participants have fewer observations at a given word 
presentation rate.
Observations were always removed at the level of item (i.e., quadruple)
For example, if one word was unintelligible, then the whole item (that is, all
four verbs in that trial) was deleted.
This results in the following number of items per participant and word 
presentation rate:


```{r}
with(memw, table(word_duration, participant))
```

The most extreme case is participant 902 under the word presentation rate of
300 ms, for whom `r 28-19` observations are missing 
(`r round(100 * (28 - 19) / 28)`%).
For now, we leave all participants in, as there is no reason to assume that this
resulted in a special bias.


### Error rates by word duration

The goal was to find a setting of word duration that would leave participants at approximately 30% error rates in the control condition tested here (see S&P, p.X).

The following figure shows mean error rates (red dots) and confidence intervals of by-subject means (red lines) for each word duration setting (x-axis). The smaller grayish dots show individual by-subject means.


```{r}
ppt_mean <- memw %>% 
  group_by(participant, block, word_duration) %>%
  summarise(error_rate = 1 - mean(score))
ylims <- c(0,1)
ggplot(ppt_mean, aes(x = word_duration, y = error_rate)) +
  stat_summary(fun.data = "mean_cl_boot", colour = "red", size = 1.5) +
  geom_jitter(width = 20, height = 0, alpha = .4) +
  ylab("Error rate") + xlab("word duration (in ms)") +
  ggtitle("Average by-subject error rates as a function of presentation rate") +
  ylim(ylims)
```

We see that L2 speakers on average had error rates at or above 50%.
Even when each word was shown for 400 ms, error rates were around 50%.
Overall, word duration did not seem to have a very strong effect.
We will follow up with more detailed analyses below.

We can also break down observations by the type of verbs (arm- vs leg-related):

```{r}
ppt_mean_ty <- memw %>% 
  group_by(participant, block, word_duration, type) %>%
  summarise(error_rate = 1 - mean(score))

ggplot(ppt_mean_ty, aes(x = word_duration, y = error_rate, colour = type)) +
  stat_summary(fun.data = "mean_cl_boot", size = 1.5, position = position_dodge(width = 20)) +
  geom_jitter(width = 20, height = 0, alpha = .4) +
  ylab("Error rate") + xlab("word duration (in ms)") +
  ylim(ylims)
```

It seems like both types of verbs were of comparable difficulty.


### Error rates by block

Perhaps more important than the word duration was the actual block number, because error rates could drop as participants proceded into the experiment and already had encountered the verbs (since the same verbs were shown in all blocks).

```{r}
ggplot(ppt_mean, aes(x = block, y = error_rate)) +
  stat_summary(fun.data = "mean_cl_boot", colour = "red", size = 1.5) +
  geom_jitter(width = .2, height = 0, alpha = .4) +
  ylab("Error rate") +
  ggtitle("Average by-subject error rates as a function of block") +
  ylim(ylims)
```

Indeed there seems to be a drop of error rates from the 1st to the 2nd block, perhaps because participants needed a block to get familiarized with the task.
Error rates did not drop much in subsequent blocks after the 2nd.

Again, the patterns were similar if we break down the verbs by type (arm- vs leg-related), as below:

```{r}
ggplot(ppt_mean_ty, aes(x = block, y = error_rate, colour = type)) +
  stat_summary(fun.data = "mean_cl_boot", size = 1.5, position = position_dodge(width = .5)) +
  geom_jitter(width = .2, height = 0, alpha = .4) +
  ylab("Error rate") + 
  ylim(ylims)
```


### Types of errors

What were the most frequent error types?

```{r}
error_types <- memw %>%
  group_by(error, error_expl) %>%
  count() %>%
  mutate(percentage = round(100 * n / nrow(memw), 1))
levels(error_types$error_expl)[levels(error_types$error_expl) == ""] <- "No error"
error_types <- error_types %>% rename(error_explicit = error_expl)
error_types <- error_types[order(- error_types$n), ]
kable(error_types[order(- error_types$n), ])
```

The table shows that most of the errors are either replacement or omission
errors (or both). These two categories *alone* (i.e., not combined with any 
other type of error) account for 
**`r round(100 * sum(error_types$n[2:4]) / sum(error_types$n[2:nrow(error_types)]))`%**
of all the items/quadruples that were not remembered correctly.


### Participant variability

The plots in the previous section suggest substantial individual variability.
Let us look at this by plotting each participant's mean error rate (averaged across blocks).

```{r}
ppt_mean_simp <- memw %>% 
  group_by(participant) %>%
  summarise(error_rate = 1 - mean(score))
ppt_mean_simp$participant <- factor(ppt_mean_simp$participant, levels = ppt_mean_simp$participant[order(ppt_mean_simp$error_rate)])
ggplot(ppt_mean_simp, aes(x = participant, y = error_rate)) +
  geom_bar(stat = "identity") +
  ylab("Error rate") +
  ggtitle("Individual variability:\nAccuracy scored at verb level") +
  ylim(ylims)
```

Variability is quite large (range: `r round(range(ppt_mean_simp$error_rate), 2)`).


#### Within-participant correlation between verb types (arm/leg)

We can ask if error rates for the two types of verbs were correlated (by participants)?

```{r}
ppt_mean_simp_type <- memw %>% 
  group_by(participant, type) %>%
  summarise(error_rate = 1 - mean(score)) %>%
  spread(type, error_rate)  # reshape to wide format for correlation plot
ggplot(ppt_mean_simp_type, aes(x = arm, y = leg)) +
  geom_point() +
  xlab("Error rate for arm items") + ylab("Error rate for leg items") + 
  geom_smooth(method = "lm")
```

The correlation is indeed high 
(*r* = `r round(with(ppt_mean_simp_type, cor(arm, leg)), 2)`),
again suggesting that participants ordered in similar ways with respect to 
error rates, independently of the verb type; that is, it was not the case that
participants ranked very high at, say, remembering arm verbs but very low at
remembering leg verbs.

Correlation test:

```{r}
with(ppt_mean_simp_type, cor.test(arm, leg))
```

To better compare memory for the two word types, we can plot average by-subject
error rates for each verb type.
The first boxplot below gives an idea of the distribution.

```{r}
ppt_mean_simp_type_long <- memw %>%
  group_by(participant, type) %>%
  summarise(error_rate = 1 - mean(score))
ggplot(ppt_mean_simp_type_long, aes(x = type, y = error_rate, colour = type)) +
  geom_boxplot() +
  ylim(ylims) +
  ylab("Error rate") +
  ggtitle("Mean by-subject error rate differentiated by type")
```


In the enhanced plot below, we superimpose individual dots showing by-subject
averages, and each pair of averages for a given participant are connected by 
a straight line:

```{r}
ggplot(ppt_mean_simp_type_long, aes(x = type, y = error_rate, colour = type)) +
  geom_boxplot() +
  geom_jitter(width = .05, height = 0) +
  geom_line(aes(group=participant), colour = "blue") +
  ylim(ylims) +
  ylab("Error rate") +
  ggtitle("Mean by-subject error rate differentiated by type")
```



#### Within-participant correlation between different word durations

We can use a scatterplot matrix to check whether the by-participant correlation between different word durations was high:

```{r}
ppt_mean_simp_durat <- memw %>% 
  group_by(participant, word_duration) %>%
  summarise(error_rate = 1 - mean(score)) %>%
  spread(word_duration, error_rate)  # reshape to wide format for correlation plot
```

```{r, eval = TRUE}
ggpairs(ppt_mean_simp_durat[, 2:5], lower = list(continuous = "smooth"))
```


Correlations are high, suggesting that, generally, a participant's error rate for a particular word duration was predictive of their error rates with another word duration.


#### Within-participant correlation between blocks

We can use the same scatterplot matrix approach to check whether the by-participant correlation between blocks also was high:

```{r}
ppt_mean_simp_block <- memw %>% 
  group_by(participant, block) %>%
  summarise(error_rate = 1 - mean(score)) %>%
  spread(block, error_rate)  # reshape to wide format for correlation plot
```

```{r, eval = TRUE}
ggpairs(ppt_mean_simp_block[, 2:5], lower = list(continuous = "smooth"))
```


And indeed it was -- even higher than the correlation by word-duration!



## By-subject analysis at the level of *individual verbs*

Instead of scoring whole items (quadruples) as correct or incorrect, we now
consider individual verbs.
If a verb within an item was remembered correctly, it is scored as correct (=1),
if it is not remembered, it gets a score of zero.
This means, for instance, that

- An item/quadruple for which 3 of the 4 verbs were remembered yields a 25%
error rate under this coding, whereas it led to a 100% error rate under the
more severe coding by item/quadruple.
- An item for which all verbs were remembered, but in a shuffled order, would
be scored as completely accurate under the verb-level coding, but as completely
wrong under the item/quadruple coding.

In short, this type of coding is more lenient and also more sensitive, but it
completely disregads errors of shuffling the order of verbs.


### Nature of the data (long format)

To look at the data by verb rather than by item, we need the data in long format:

```{r}
# Show data in long format
kable(head(meml))
```

Notice now each row corresponds to a single verb.
For instance, the first 4 rows of the table above show the 4 verbs from the 
first item of the first block, which were all remembered correctly by 
participant 900.
This participant did not, however, remember the 2nd word (`wordIntTrial` = 2, namely *clutch*)
in trial 2.


### Error rates by word duration

We begin with a plot equivalent to the one we did for whole items, but now we show the error rates when we consider whether individual verbs were remembered.

```{r}
ppt_mean_l <- meml %>% 
  group_by(participant, block, word_duration) %>%
  summarise(error_rate = 1 - mean(correct))
ggplot(ppt_mean_l, aes(x = word_duration, y = error_rate)) +
  stat_summary(fun.data = "mean_cl_boot", colour = "red", size = 1.5) +
  geom_jitter(width = 20, height = 0, alpha = .4) +
  ylab("Error rate") + xlab("word duration (in ms)") +
  ylim(ylims)
```

Under this treatment, error rates diminish drastically -- they are halved!
We now lie at a mean error rate below 25% and no participant has a mean error
rate above 50%.
The effect of word duration becomes less clear.
But let us rescale the y-axis so it covers the range of the data
rather than forcing the y-scale to lie between 0 and 1. 
This yields better resolution to see variability.


```{r}
ylims2 <- c(0, .55)
ggplot(ppt_mean_l, aes(x = word_duration, y = error_rate)) +
  stat_summary(fun.data = "mean_cl_boot", colour = "red", size = 1.5) +
  geom_jitter(width = 20, height = 0, alpha = .4) +
  ylab("Error rate") + xlab("word duration (in ms)") +
  ylim(ylims2)
```


We can also break down observations by the type of verbs (arm- vs leg-related).

```{r}
ppt_mean_l_ty <- meml %>% 
  group_by(participant, block, word_duration, type) %>%
  summarise(error_rate = 1 - mean(correct))
ggplot(ppt_mean_l_ty, aes(x = word_duration, y = error_rate, colour = type)) +
  stat_summary(fun.data = "mean_cl_boot", size = 1.5, position = position_dodge(width = 20)) +
  geom_jitter(width = 20, height = 0, alpha = .4) +
  ylab("Error rate") + xlab("word duration (in ms)") +
  ylim(ylims2)
```

It seems like, under this analysis as well, both types of verbs were of comparable difficulty.


### Error rates by block

Was there a difference between the different blocks?

```{r}
ggplot(ppt_mean_l, aes(x = block, y = error_rate)) +
  stat_summary(fun.data = "mean_cl_boot", colour = "red", size = 1.5) +
  geom_jitter(width = .2, height = 0, alpha = .4) +
  ylab("Error rate") +
  ylim(ylims2)
```

We see the same picture as before:
Error rates drop from the 1st to the 2nd block, but then remain stable throughout
the rest of the blocks.

This does not differ between verb types, as shown below:

```{r}
ggplot(ppt_mean_l_ty, aes(x = block, y = error_rate, colour = type)) +
  stat_summary(fun.data = "mean_cl_boot", size = 1.5, position = position_dodge(width = .4)) +
  geom_jitter(width = .2, height = 0, alpha = .4) +
  ylab("Error rate") +
  ylim(ylims2)
```


### Effect of verb position within an item

Was the position in which a verb occured in the item (word 1--4) predictive of how well it was remembered?
This seems likely, as we know that in lists the first and last items are better remembered than those in the middle.

```{r}
ppt_mean_l_wordInTrial <- meml %>% 
  group_by(participant, wordInTrial) %>%
  summarise(error_rate = 1 - mean(correct))

ggplot(ppt_mean_l_wordInTrial, aes(x = wordInTrial, y = error_rate)) +
  stat_summary(fun.data = "mean_cl_boot", size = 1.5) +
  geom_jitter(height = 0, width = .2, alpha = .4) +
  ylab("Error rate") + xlab("Verb position") +
  ylim(ylims2)
```

The position of the verb indeed seems to matter:
the first verb in a quadruple tended to be better remembered than any of the
following. 
But performance was not better for the last one than for the middle ones,
as one might have expected.

We can further investigate whether this effect changed across word presentation
rates:

```{r}
ppt_mean_l_wordInTrial_durat <- meml %>% 
  group_by(participant, wordInTrial, word_duration) %>%
  summarise(error_rate = 1 - mean(correct))
ggplot(ppt_mean_l_wordInTrial_durat, aes(x = wordInTrial, y = error_rate)) +
  stat_summary(fun.data = "mean_cl_boot", size = 1.5) +
  geom_jitter(height = 0, width = .2, alpha = .4) +
  facet_grid(. ~ word_duration) +
  ylab("Error rate") + xlab("Verb position")  +
  ylim(0, .6)
```

No interaction between effect of verb position and verb presentation rate
is visible

... What about across blocks?

```{r}
ppt_mean_l_wordInTrial_block <- meml %>% 
  group_by(participant, wordInTrial, block) %>%
  summarise(error_rate = 1 - mean(correct))
ggplot(ppt_mean_l_wordInTrial_block, aes(x = wordInTrial, y = error_rate)) +
  stat_summary(fun.data = "mean_cl_boot", size = 1.5) +
  geom_jitter(height = 0, width = .2, alpha = .4) +
  facet_grid(. ~ block) +
  ylab("Error rate") + xlab("Verb position")  +
  ylim(0, .6)
```

The effect does not seem to change much between blocks either.
(As before, there is only an overall shift downwards from block 1 to block 2,
but the relative position of the means depending on word position is
similar).



### Participant variability

Let us consider if the picture for participant variability changes when looking
at it by individual verbs rather than quadruples.
We plot each participant's mean error rate averaged across blocks:

```{r}
ppt_mean_l_simp <- meml %>% 
  group_by(participant) %>%
  summarise(error_rate = 1 - mean(correct))
ppt_mean_l_simp$participant <- factor(ppt_mean_l_simp$participant, levels = ppt_mean_l_simp$participant[order(ppt_mean_l_simp$error_rate)])
ggplot(ppt_mean_l_simp, aes(x = participant, y = error_rate)) +
  geom_bar(stat = "identity") +
  ylab("Error rate") +
  ggtitle("Individual variability:\nAccuracy scored at verb level") +
  ylim(ylims)
```

Participant variability is much reduced when error rates are computed by 
individual verbs. The range is: `r round(range(ppt_mean_l_simp$error_rate), 2)`.
For comparison, we plot again variability when computed by items
(range `r round(range(ppt_mean_simp$error_rate), 2)`):

```{r}
ggplot(ppt_mean_simp, aes(x = participant, y = error_rate)) +
  geom_bar(stat = "identity") +
  ylab("Error rate") +
  ggtitle("Individual variability:\nAccuracy scored at quadruple level") +
  ylim(ylims)
```


We can see if the two ways of measuring error rates correlate well within 
participants:

```{r, message=FALSE}
# join the two tables
ppt_mean_simp$participant <- as.character(ppt_mean_simp$participant)
ppt_mean_l_simp$participant <- as.character(ppt_mean_l_simp$participant)
error_rates <- left_join(ppt_mean_simp, ppt_mean_l_simp %>%
                           rename(error_rate_verb = error_rate))
```

```{r}
# plot correlation
ggplot(error_rates, aes(x = error_rate_verb, y = error_rate)) +
  geom_point() +
  xlab("Error rate for individual verbs") + ylab("Error rate for items (quadruples)") + 
  geom_smooth(method = "lm")
```

The correlation is very high 
(*r* = `r round(with(error_rates, cor(error_rate_verb,error_rate)), 2)`),
suggesting that computing errors in one way or another does not order participants
differently.
This is good news, as we may then pretty safely use the error rate by verbs,
which is a more sensitive measure.



## Verb analysis

We can use participant performance on the memory task to learn something about
the difficulty of the verbs.
To do that, we look at whether individual verbs (not items) were remembered 
correctly.

First, here is a list of the verbs we used in this pilot:

- **Arm-related verbs**: `r sort(unique(meml[meml$type == "arm", "verb"]))`
- **Leg-related verbs**: `r sort(unique(meml[meml$type == "leg", "verb"]))`

Note that some observations were removed because participants did something
they were not supposed to (e.g., whispering the words during the memory phase)
or because the answers were not perceivable.
This leads to some verbs having 1 or 2 missing observations for a given word
presentation rate.

```{r}
missing_obs_per_verb <- meml %>% 
  group_by(verb, word_duration) %>%
  summarise(missing_observations = 17 - n())
missing_obs_per_verb[missing_obs_per_verb$missing_observations != 0, 2:3] %>%
  count(word_duration, missing_observations) %>%
  kable
```

The table above should be read as follows:

- Row 1: There are 8 verbs for which there is one missing observation (i.e. there is 
data for 16 out of 17 participants) for word_duration 100 ms.
- Row 2: There are 12 verbs for which there is one missing observation (i.e. there is 
data for 16 out of 17 participants) for word_duration 200 ms
- etc.


### Item variability

The average error rate of an item is the mean number of participants who 
remembered it (averaged across blocks):

```{r, fig.width=8}
item_mean_simp <- meml %>% 
  group_by(verb, type) %>%
  summarise(error_rate = 1 - mean(correct))
item_mean_simp$verb <- factor(item_mean_simp$verb, levels = item_mean_simp$verb[order(item_mean_simp$error_rate)])
ggplot(item_mean_simp, aes(x = verb, y = error_rate, fill = type)) +
  geom_bar(stat = "identity") +
  ylab("Error rate") +
  theme(axis.text.x = element_text(angle = 90, hjust = 1, size = 8)) +
  ylim(ylims2)
```

Variability by items is similar in magnitude to variability between participants
when this latter is computed for error rate at the verb level
(range: `r round(range(item_mean_simp$error_rate), 2)`).

Another nice result this plot confirms is that there does not seem to a marked
difference in difficulty between the two types of verbs:
red and blue bars are spread out quite evenly along the whole range.
Let us verify this with a boxplot:

```{r}
ggplot(item_mean_simp, aes(x = type, y = error_rate, colour = type)) +
  geom_boxplot() +
  ylab("Error rate") 
```

Indeed, the two verb types elicited very similar error rates.


We can have a look at the easiest and hardest verbs to remember (of each type):

```{r}
my_order <- function(df = item_mean_simp, mytype = NULL, nb = 5) {
  d <- df[df$type == mytype, ]
  d <- d[order(d$error_rate), ]
  d$error_rate <- round(d$error_rate, 2)
  print(paste("The ", nb, " easiest ", mytype, "-verbs:", sep = ""))
  print(kable(head(d, nb)))
  cat("\n\n\n")
  print(paste("The ", nb, " hardest ", mytype, "-verbs:", sep = ""))
  print(kable(tail(d, nb)))
}
my_order(mytype = "arm")
my_order(mytype = "leg")
```


### By-item correlation of error rates between different word presentation rates

We use a scatterplot matrix to check whether the by-item correlations of error
rates under different word durations were high:

```{r}
item_mean_simp_durat_wide <- meml %>% 
  group_by(verb, type, word_duration) %>%
  summarise(error_rate = 1 - mean(correct)) %>%
  spread(word_duration, error_rate)  # reshape to wide format for correlation plot
```

```{r, eval = TRUE}
# ggpairs distinguishing between verb types
# (for setting binwidth in histograms, see
# https://github.com/ggobi/ggally/issues/184 and vignette
# http://ggobi.github.io/ggally/#function_wrapping )
ggpairs(item_mean_simp_durat_wide[, 2:6],
        mapping = ggplot2::aes(colour = type),
        lower = list(continuous = "smooth", 
                     combo=wrap("facethist", binwidth = 0.1)))
# # Or a simpler version that does not distinguish between verb types
# ggpairs(item_mean_simp_durat_wide[, 3:6], lower = list(continuous = "smooth"))
```


Not sure how surprising this is, but the correlations of by-item means are
much lower than those for by-subject means.
In other words, the fact that a given verb was hard to remember for one 
presentation rate does not imply that it was equally hard at another rate.


## Conclusion from memory task

A

- The analysis based on rating whole items (quadruples) as either correct or
incorrect resulted in a high error rate (around 50%) at all verb presentation
rates.
- This rate is higher than we were aiming for based on the original study S&P, for
whom error rates in the control condition (with 100 ms verb presentation rates)
lay at around 30%.
- Manipulating the presentation rate led at best to a marginally better
performance for a 400 ms presentation rate.

- No differences by verb type


- word_duration: not much effect
- individual items: ... + we're going to follow it up now in the analysis of the following two tasks


- Omission and replacement the most frequent errors


Verb norming task: arm vs leg bias
==================================

The data from the verb norming task look like this:

```{r}
bias_l <- read.csv("data_verb-bias.csv", fileEncoding = "UTF-8")
kable(head(bias_l[, 3:8]))
```

Each participant rated each of the 112 verbs on a arm-relatedness and
a leg-relatedness scale from 1 to 7.

Unfortunately, there was a mismatch in the verbs chosen for the memory and the
norming task, so that *trek* (a leg verb used in the memory task) was 
mistakenly replaced by *pluck* (an arm verb) in the norming task.
This leads to a slight imbalance in the types of verbs used for norming:

```{r}
table(unique(bias_l[, c("verb", "type")])$type)
```


## Verb bias

We can compute a single measure of *arm-minus-leg bias* per verb
by simply substracting the leg rating from the verb rating,
obtaining:

```{r}
bias <- bias_l %>%
  group_by(verb, type, rated_category) %>%
  summarise(rating = mean(rating)) %>%
  spread(rated_category, rating) %>%  # reshape to wide format for correlation plot
  mutate(bias = Arm - Leg)
kable(head(bias))
```

- A positive bias score (bias $> 0$) indicates bias towards arm-relatedness (max $6$)
- A negative bias score (bias $< 0$) indicates bias towards leg-relatedness (min $-6$)
- A bias score $= 0$ indicates absence of either bias

```{r}
# order factor levels of verbs depending on their bias (for plotting)
ordered_verbs_bias <- as.character(pull(bias[order(bias$bias), ], verb))
bias$verb <- factor(bias$verb, levels = ordered_verbs_bias)
```


```{r, fig.height = 8, fig.width = 8}
ggplot(bias, aes(x = verb, y = bias, colour = type)) +
  geom_point() +
  geom_hline(yintercept = 0) +
  theme(axis.text.x = element_text(size = 6)) +
  coord_flip()
```

The figure shows that the bias was generally stronger for the verbs we had
categorized as arm verbs than for those we thought of as leg verbs.

This can be shown even clearer if we just consider the size of the bias in
the expected direction, so a negative bias now constitutes a bias in the 
"wrong" direction (e.g., an arm bias for a supposedly leg-related verb).
The following plot shows this:

```{r}
# We add a bias column that changes the sign for leg verbs, so that bias
# is expressed with respect to the expected direction of the bias for a verb
bias$bias_absol <- bias$bias
bias[bias$type == "leg", "bias_absol"] <- - bias[bias$type == "leg", "bias"]
ggplot(bias, aes(x = type, y = bias_absol, colour = type)) + 
  geom_boxplot() +
  ylab("Size of bias in predicted direction")
```

Only one verb had opposite bias to what was expected:

```{r}
kable(bias[bias$bias_absol < 0, ])
```

To scuff means to "Scrape or brush the surface of (a shoe or other object) 
against something" (OED).
But participants seemed not to know the verb very well and assume it had some
arm-related meaning.


## Cut-off at bias = 3

Somewhat arbitrarily, we could say that a bias of 3 in the expected direction
constitutes an acceptable cut-off point to select a verb for the study.
There are 
`r sum(with(bias, bias_absol >= 3 & type == "arm"))`
arm verbs (i.e., all but one) that satisfy this condition,
but only
`r sum(with(bias, bias_absol >= 3 & type == "leg"))`
leg verbs (out of
`r sum(with(bias, type == "leg"))`) that qualify.

Here are the leg verbs that have a clear bias:

```{r}
ggplot(bias[with(bias, bias_absol >= 3 & type == "leg"), ],
       aes(x = verb, y = bias)) +
  geom_point() +
  theme(axis.text.x = element_text(size = 6)) +
  coord_flip() +
  ggtitle("Leg verbs with strongest leg bias")
```


## Spread of ratings

Above we have only considered the mean rating for each verb (averaged across
participants).
But how much spread was there in the ratings from individual participants?

```{r}
bias_spread <- bias_l %>%
  select(participant, verb:rating) %>%
  spread(rated_category, rating) %>%
  mutate(bias = Arm - Leg)
# order verbs according to bias
bias_spread$verb <- factor(bias_spread$verb, levels = ordered_verbs_bias)
```


```{r, fig.height = 9}
ggplot(bias_spread, aes(x = verb, y = bias, colour = type)) +
  geom_hline(yintercept = 0) +
  stat_summary(fun.data = "mean_cl_boot") +
  geom_point(alpha = .4) +
  ylab("Verb bias") + 
  coord_flip()
```

What this figure shows is that the average bias for a verb can still hide some
important individual differences.
For instance, the leg verb *kneel* had a fairly strong leg bias of
`r round(bias[bias$verb == "kneel", ]$bias, 1)`
and still there was one participant who rated it as completely arm-biased
(bias = 6).

Some of these data points might simply be due to participants getting the 
scales mixed up, but probably not all of these data points came about in this
way:
Similar observations hold for the supposedly leg verbs *tread* and *bounce*.
For the latter, one could imagine how a participant made associations with 
*bouncing a ball*, in which case the verb *to bounce* naturally becomes 
arm-related).

Finally, note also the many ratings of verbs as having no bias (verb bias = 0).
These may be the result of participants not knowing a verb meaning,
a matter to which we now turn.



Verb comprehension task
======================

## Multiple choice version of verb comprehension task 

```{r}
# "und" for understanding, "mc"" for multiple choice
und_mc <- read.csv("data_verb-understanding_multiple-choice.csv",
                    fileEncoding = "UTF-8")
```

Most of the participants 
(`r length(unique(und_mc$participant))` out of 17)
carried out the verb comprehension task as a **multiple
choice task**, in which they had to choose one among three Swedish alternatives
as the correct translation for the English target verb.

Here are the first couple of observations:

```{r}
kable(head(und_mc[, c(3:5, 15, 6:8, 14)], 2))
```

The column `verb` shows the English verb for which participants had to choose
the corresponding Swedish verb (`target`), followed by the two distractors
(`r names(und_mc)[7:8]`).
The last column shows whether the participant got it right.
The order in which target and distractors appeared in a given trial was 
randomized for each verb (but each verb always had the same distractors).
The order of verbs was randomly chosen.

```{r}
und_mc_acc <- und_mc %>%
  group_by(verb, type) %>%
  summarise(accuracy = mean(correct))
```


```{r}
# order factor levels of verbs depending on their accuracy (for plotting)
ordered_verbs_acc <- as.character(pull(und_mc_acc[order(und_mc_acc$accuracy), ], verb))
und_mc_acc$verb <- factor(und_mc_acc$verb, levels = ordered_verbs_acc)
```


```{r, fig.height = 9}
ggplot(und_mc_acc, aes(x = verb, y = accuracy, colour = type)) +
  geom_point() +
  theme(axis.text.x = element_text(size = 6)) +
  ylim(0,1) +
  coord_flip()
```

In general, translation of leg verbs yielded lower accuracy than arm verbs:

```{r}
ggplot(und_mc_acc, aes(x = type, y = accuracy, colour = type)) +
  geom_boxplot() +
  ylim(0, 1) +
  ggtitle("Accuracy for arm vs leg verbs (multiple choice)")
```



Methodologically, this task might be problematic:
As I had initially feared, the multiple choice task perhaps is not sensitive
enough. Many of the verbs had an accuracy of 100%.
This may be because they truly are well known, but it could also be that 
participants were able to rule out the distractors because they were too far
away from the meaning of the English verb, even though their actual knowledge
of the verb could have been shallow.
Noise in the opposite direction is also a possibility for other verbs:
perhaps some verbs *were* known, but the alternative give as the correct one
was not really ideal.
This is difficult to tell but we can look a little bit more into this matter.

There are some verbs for which accuracy was particularly low, even under chance
level (.33).
The verbs that had an accuracy below 0.5 are the following:


```{r}
# difficult (?) verbs
diff_verbs <- und_mc_acc[und_mc_acc$accuracy < .5, ]
kable(diff_verbs[order(diff_verbs$accuracy), ])
# save as vector
diff_verbs_v <- as.character(diff_verbs[order(diff_verbs$accuracy), ]$verb)
```


This is surprising, especially for "roll," since one would not expect Swedish
speakers of L2 English not to know the meaning of that verb.

So let us look at the target and distractor items for these verbs:

```{r}
kable(unique(und_mc[und_mc$verb %in% diff_verbs_v, c("verb", "target", "distrOne", "distrTwo")]))
```

It may very well be that rather than not knowing the word, some of the
participants could not make sense of the different alternatives.



## Free (oral) translation version of verb comprehension task 




```{r}
# "und" for understanding, "fr"" for free
und_free <- read.csv("data_verb-understanding_free-translation.csv",
                     fileEncoding = "UTF-8")
```

Only  
(`r length(unique(und_free$participant))`
participants out of 17 carried out the verb comprehension task as a 
**free translation task**, in which they had to give a correct Swedish 
translation of the English target verb.

Here are the first couple of observations:
  
```{r}
kable(head(und_free[, c(3:5, 11, 7:9)], 2))
```

The column `verb` shows the English verb for which participants had to provide
a Swedish translation.
The column `ppt_translation` contains a transcription of the participant's answer
which is scored as 0 or 1, except in cases where we were in doubt.
These latter cases are coded as $-99$, e.g.:

```{r}
kable(head(und_free[und_free$score == -99, c(3:5, 11, 7:9)], 3))

```

We coded cases as $-99$ when the translations were not incorrect but clearly
deviated from the intended translation and lacked any relation to legs or 
arms as we had intended.

For now we will consider translations scored as $-99$ as incorrect
(but we also save a version of the non-simplified scoring):

```{r}
und_free$score_unsimplified <- und_free$score
und_free[und_free$score == -99, "score"] <- 0
```


```{r}
und_free_acc <- und_free %>%
  group_by(verb, type) %>%
  summarise(accuracy = mean(score))
```


```{r}
# order factor levels of verbs depending on their accuracy (for plotting)
ordered_verbs_free_acc <- as.character(pull(und_free_acc[order(und_free_acc$accuracy), ], verb))
und_free_acc$verb <- factor(und_free_acc$verb, levels = ordered_verbs_free_acc)
```


```{r, fig.height = 9}
ggplot(und_free_acc, aes(x = verb, y = accuracy, colour = type)) +
  geom_point() +
  theme(axis.text.x = element_text(size = 6)) +
  ylim(0,1) +
  coord_flip()
```

In general, translation of leg verbs yielded lower accuracy than arm verbs,
which was also the case in the multiple choice version of this task:
  
```{r}
ggplot(und_free_acc, aes(x = type, y = accuracy, colour = type)) +
  geom_boxplot() +
  ylim(0, 1) +
  ggtitle("Accuracy for arm vs leg verbs (free translation)")
```


There are some verbs for which accuracy was particularly low:

```{r}
# difficult (?) verbs
diff_verbs_free <- und_free_acc[und_free_acc$accuracy <= .5, ]
kable(diff_verbs_free[order(diff_verbs_free$accuracy), ])
```

Some of the verbs really were unknown to the 6 participants in the free
translation task, e.g.:
*ramble* or *scuff*.
Or only 1 participant knew them (*traipse*, *flit*).

For others, like *switch*, the participants provided a translation that was 
unrelated to the intended meaning (like 'byta').


## Correlation between the two verb comprehension tasks

Even though we have only few observations, we can look at the correlation
of verb comprehension accuracy in the two tasks:

```{r, message = FALSE}
und_free_acc$verb <- as.character(und_free_acc$verb)
und_mc_acc$verb <- as.character(und_mc_acc$verb)
# join information from free translation and multiple choice versions
und_compare <- left_join(und_mc_acc %>% rename(MC_acc = accuracy),
                         und_free_acc %>% rename(free_acc = accuracy))
```

We plot first the correlation plot for all verbs...

```{r}
ggplot(und_compare, aes(x = MC_acc, y = free_acc)) +
  geom_jitter(height = .02, width = .02, alpha = .4) +
  xlab("Multiple choice") + ylab("Free translation") + 
  ggtitle("Translation accuracy in multiple choice and free translation task") +
  geom_smooth(method = "lm")
```

And then we break up the plot as a function of verb type:

```{r}
ggplot(und_compare, aes(x = MC_acc, y = free_acc, colour = type)) +
  geom_jitter(height = .02, width = .02, alpha = .4) +
  xlab("Multiple choice") + ylab("Free translation") + 
  ggtitle("Translation accuracy in multiple choice and free translation task") +
  geom_smooth(method = "lm")
```



Choice of verbs
===============


## Combine and visualize all information about the verbs

We need to combine all the information about the individual verbs:

- How well they were remembered (memory task)
- How strong a bias they elicited (verb norming task)
- How well they were understood (multiple-choice and free-translation versions
of the comprehension task)


(To combine this info, here is a very useful cheat sheet for 
[dplyr join functions](http://stat545.com/bit001_dplyr-cheatsheet.html)):

```{r, message = FALSE, warning = FALSE}
# contains error_rates: item_mean_simp
# contains verb bias: bias (we need only columns: verb, type, bias_absol)
# contains verb comprehension (multiple choice & free translation): und_compare

# because there are verbs in the memory task that are not in the other tasks
# and viceversa, what we need is a full_join()!
# See useful cheat sheet for dplyr join functions:
# http://stat545.com/bit001_dplyr-cheatsheet.html
verb_info <- full_join(full_join(item_mean_simp,
                                 bias %>% select(verb, type, bias_absol)),
                       und_compare)
```

First of all we look at a scatterplot matrix that displays the correlation
between the different variables without distinguishing by verb type.

```{r, warning = FALSE, fig.height = 6}
# A simple version that does not distinguish between verb types
ggpairs(verb_info[, 3:6], lower = list(continuous = "smooth"))
```

The information we can extract from this plot is quite interesting.
Let us read the plot column by column (i.e. we start with the plot in the upper
left corner, then we move down, etc.).
For convenience, we will identify each plot's position with the tuple
[row, column], (e.g., [2,3] is the plot that displays the correlation 
coefficient of 0.392).

- Plot [1,1]: The histogram of the variable `error_rate` shows a relatively normal-
looking distribution.
- Plots [2,1]--[4,1] all show that the error rate for a particular verb did not 
strongly correlate with either its bias ([2,1]) or its translation accuracy
in any of the two translation tasks. The numerical values of these correlations
are shown in panels [1,2]--[1,4].
- Plot [2,2] shows that the bias scores tended to be close to the upper limit of
6, which is of course good since we want the verbs to elicit a bias.
- Plots [3,2] and [4,2] show that the bias strength was positively correlated
with translation accuracy, especially so with accuracy in the free translation
task ($r = .554$). This makes sense because verbs that are not well understood
should also lead to a low absolute bias.
- Finally [4,3] shows the correlation between the two verb comprehension tasks,
something we already looked at above.


We gain further insight by breaking up the previous scatterplot matrix
by verb type (arm or leg):

```{r, warning = FALSE, fig.height = 8}
ggpairs(verb_info[, 2:6],
        mapping = ggplot2::aes(colour = type),
        lower = list(continuous = "smooth", 
                     combo=wrap("facethist", binwidth = 0.1)))
```

The interesting trend we observe when we do that is found in plots
[3,2]--[5,2]. They show that error rates for arm verbs did not show
any correlation with any of the other variables. 
In contrast, error rates for leg verbs tended to increase when its absolute
bias was lower or when the translation accuracy was lower.


## What verbs to choose?

To choose the optimal verbs, one has to take into consideration both the bias
and the translation accuracy.
We have seen, however, that a verb that is not well understood also
tends to have a low absolute bias.
Conversely, a verb that has a strong absolute bias is likely to be well 
understood.
Since the translation accuracy data is problematic (methodologically so for 
the multiple choice version and because of low N for the free translation 
version), let us focus on the verbs that had highest bias and just check that
their accuracy was also acceptable.


### Verbs ranked according to strength of bias

The following table ranks arm and leg verbs with respect to their absolute
bias.
For example the strongest arm bias was obtained for the verb *clap*,
while the strongest leg bias was obtained for the verb *kick*.
Along with the verb and the bias, the table shows the translation accuracy
of that verb as measured from the **free translation task**.

```{r, message = FALSE, warning = FALSE}
# convenience function to rank verbs
rank_verbs_fnc <- function(df_bias = bias, df_acc = und_free_acc,
                           min_acc = 0, min_bias = -Inf) {
  # The min_acc parameter is used to threshold verbs for their accuracy:
  # verbs with translation accuracy < min_acc are excluded
  # min_bias thresholds verbs for their absolute bias
  
  for (ty in c("arm", "leg")) {
    bias <- df_bias[df_bias$type == ty, c("verb", "type", "bias_absol")]
    bias <- left_join(bias, df_acc)  # add accuracy from free translation
    bias <- bias[with(bias, order(-bias_absol, -accuracy)),]
    bias <- bias[with(bias, accuracy >= min_acc), ]  # accuracy threshold
    bias <- bias[with(bias, bias_absol >= min_bias), ]  # bias threshold
    bias$rank <- 1:nrow(bias)
    # reorganize columns
    bias <- bias %>%
      ungroup() %>%
      select(rank, verb, bias_absol, accuracy)
    # rename columns
    names(bias)[2:4] <- paste(ty, c("verb", "bias", "acc"), sep = "_")
    assign(ty, bias)
  }
  out <- full_join(arm, leg)
  out
}
verb_choice_nothresh <- rank_verbs_fnc()
```

```{r}
kable(verb_choice_nothresh)
```


### Verbs with threshold for accuracy

The ranks above do not take into account accuracy.
One reasonable thing to do is to exclude verbs that were accurately translated
by less than 50% of the 6 participants in the free translation task.
This new list of ranked verbs does that:

```{r, message = FALSE, warning = FALSE}
acc_threshold <- .5
verb_choice_thresh50 <- rank_verbs_fnc(min_acc = acc_threshold)
kable(verb_choice_thresh50)
```


Note that we are left with many less leg than arm verbs!


The verbs excluded in this way are the following: 

```{r}
und_free_acc %>% filter(accuracy < acc_threshold) %>%
  arrange(accuracy, type, verb) %>% kable
```



### Verbs with threshold for accuracy and strength of bias

If we additionally impose the condition that the 
**minimum bias of a verb should be 3** in the expected direction, 
we end up with the following list of ranked verbs:

```{r, message = FALSE, warning = FALSE}
bias_threshold <- 2.5
verb_choice_thresh50_bias3 <- rank_verbs_fnc(min_acc = acc_threshold,
                                             min_bias = bias_threshold)
kable(verb_choice_thresh50_bias3)
```

We have 54 arm verbs, but only 37 leg verbs, that satisfy these constraints!



Appendix: Logistic mixed model analyses 
========

## Memory task: accuracy rated at item/quadruple level

To analyze the data statistically, we fit a logistic mixed model.

The dependent variable is binary (`score`).
The predictor variables are:

- `word_duration` dummy coded (`r levels(factor(memw$word_duration))` ms)
- `block` dummy coded (1, 2, 3, 4)
- `type` dummy coded (`r levels(factor(memw$type))`)

We can only have participants as random effects, including random intercepts
as well as (at most) random slopes for all three predictors.
As we will see below, random slopes lead to convergence failures.
No random effects for items can be fit, since the verbs that made up
the quadruples were randomly chosen each time.


```{r, echo = TRUE}
# convert variables to factors:
memw$f_word_duration <- factor(memw$word_duration)
memw$f_block <- factor(memw$block)

# contrast coding for verb type
contrasts(memw$type) <- contr.sum(2) / 2
colnames(contrasts(memw$type)) <- "arm_vs_leg"
# use forward difference coding, see
# https://stats.idre.ucla.edu/r/library/r-library-contrast-coding-systems-for-categorical-variables/
# In this coding system, the mean of the dependent variable for one level of
# the categorical variable is compared to the mean of the dependent variable
# for the next (adjacent) level.

#forward difference for factor variable with 4 levels
my.forward.diff <- matrix(c( 3/4, -1/4, -1/4,
                            -1/4,  1/2,  1/2, 
                            -1/2, -1/2,  1/4,
                             1/4,  1/4, -3/4), 
                          ncol = 3)
# word presentation rate
contrasts(memw$f_word_duration) <- my.forward.diff
colnames(contrasts(memw$f_word_duration)) <- c("100_vs_200",
                                               "300_vs_200",
                                               "400_vs_300")
# block
contrasts(memw$f_block) <- my.forward.diff
colnames(contrasts(memw$f_block)) <- c("1_vs_2", "3_vs_2", "4_vs_3")

# History of fitted models:

# # The maximal model fails to converge:
# glmm_quadr <- glmer(score ~ f_word_duration + f_block + type +
#                       (1 + f_word_duration + f_block + type | participant),
#                     data = memw,
#                     family = 'binomial')
# # So does a model removing the (nuisance) variable block
# glmm_quadr <- glmer(score ~ f_word_duration + f_block + type +
#                       (1 + f_word_duration + type | participant),
#                     data = memw,
#                     family = 'binomial')
# #  Even if we just leave f_word_duration as a random slope, it doesn't converge.
# glmm_quadr <- glmer(score ~ f_word_duration + f_block + type +
#                       (1 + f_word_duration | participant),
#                     data = memw,
#                     family = 'binomial')

# So we go for a random by-subject intercept only:
glmm_quadr.expr <- "glmer(score ~ f_word_duration + f_block + type +
                      (1 | participant),
                    data = memw,
                    family = 'binomial')"
# load if saved or fit it
load_or_fit("glmm_quadr", glmm_quadr.expr)
summary(glmm_quadr)
```


Compare to dummy coding of `word_duration` and `block`:

```{r}
# Same model using dummy coding
glmm_quadr_dummy.expr <- "glmer(score ~ factor(word_duration) + factor(block) + type +
                            (1 | participant),
                          data = memw, family = 'binomial')"
# load if saved or fit it
load_or_fit("glmm_quadr_dummy", glmm_quadr_dummy.expr)

summary(glmm_quadr_dummy)
```


## Memory task: accuracy rated at the level of individual verbs

