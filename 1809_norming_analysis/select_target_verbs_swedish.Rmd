---
title: "Selection of Swedish target verbs"
author: '[Guillermo Montero-Melis](https://www.mpi.nl/people/montero-melis-guillermo)'
date: '`r as.character(format(Sys.Date(), format="%d/%m/%Y"))`'
output:
  html_document:
    depth: 2
    number_sections: yes
    theme: default
    toc: yes
---



Introduction
============

This script selects the Swedish verbs that will be used in the replication of
Shebani and Pulvermüller (2013, Cortex, henceforth SP13).
Because we had to control for the same variables as in the original study,
we needed to collect quite a lot of norms for the Swedish verbs.

**NB**:
There is a previous report in this gitrepo 
("1809_norming_analysis/norming_analysis.Rmd") that does many of the pre-selection
on which the current report builds (see for details). The present report/script
focuses on the final selection of Swedish target verbs.


Goal
====

We want to select an equal amount of leg- and arm-related verbs that differ
maximally in their arm/leg relatedness, while they are matched for the
following variables:

- Number of letters
- Number of phonemes
- Word frequency 
- Grammatical ambiguity 
- Lemma frequency 
- Bigram frequency 
- Trigram frequency 
- Valence 
- Arousal
- Imageability 

Note that SP13 mention three additional variables in Table 1, p.225:
Visual relatedness, body relatedness, and action relatedness. We checked with
the correspondin author, Friedemann Pulvermüller, and it turns out these
measures were actually redundant, so he acknowledged we did not need to collect
them (see correspondence from Tue 2019-04-30, 10:19 PM; subject "Short query
about word norms in Shebani & Pulvermüller (2013)").



Set up workspace
================

Libraries
---------

```{r setup, include=TRUE}
library(knitr)
library(ggplot2)
library(ggrepel)
# library(GGally)
library(dplyr)
library(tidyr)
library(readr)  # Improves the base functions for reading data files.
library(purrr)
# library(lme4)
knitr::opts_chunk$set(echo = TRUE, fig.height = 3.5)
```

Data import and processing
---------------------

We want to sequentially integrate all relevant variables into the same dataframe.

### Arm/leg bias

```{r, include = TRUE}
## load data files
# Arm/leg verb bias Swedish (note this already contains average ratings, for
# details about how it was generated, see "norming_analysis.Rmd")
bias <- read_csv("verb-bias_sw_mean.csv")
head(bias)
```

### Norms (imageability, valence and arousal ratings)

```{r}
# verb norming Swedish: imageability, valence and arousal ratings
# (this processed file is created in "norming_imageability-etc_preprocess.R")
norms <- read_csv("data_verb-norming_swe_ratings.csv")
head(norms)
```


Among the verbs in `norms` we have arm verbs, leg verbs and control verbs.
Let's add that info:

```{r}
# arm/leg words
norms <- left_join(norms, bias %>% select(Word = verb, Category = category))
# the rest are control words
norms[is.na(norms$Category), "Category"] <- "control"
norms$Category <- factor(norms$Category, levels = c("arm", "leg", "control"))
head(norms)
```

We only want mean ratings:

```{r}
norms_mean <- norms %>%
  filter(Category != "control") %>% 
  group_by(Word, Category, Task) %>%
  summarise(Rating = mean(Rating))
norms_mean$Category <- factor(norms_mean$Category)
```


### Combine norms with bias ratings into single `stim` file

It's easiest to have a wide data set where each column is one property of
the stimuli:

```{r}
norms_mean <- spread(norms_mean, Task, Rating)
head(norms_mean, 4)
```



```{r}
stim <- left_join(norms_mean, bias %>% select(Word = verb, Arm : bias_absol))
head(stim, 4)
```



### Word frequency

The different frequency measures were obtained from Jeroen vP's big Sub2vec 
Swedish corpus with his help:

The frequency of the actual word form (infinitives):

```{r}
freq_word <- read_tsv(
  "frequency_measures/filtered_word_unigram_freqs_sv.tsv", col_names = FALSE
  ) %>%
  rename(Word = X1, word_freq = X2) %>%
  mutate(word_logfreq = log10(word_freq))
head(freq_word)
```


Add to `stim`

```{r}
stim <- left_join(stim, freq_word %>% select(-word_freq))
head(stim, 4) %>% kable
```



### Lemma frequency

The frequency of the lemma word forms can be computed in different ways
(see Brysbaert and New, 2009, p.982-984). I define it here as the (logged) sum
of the frequencies of all forms of the word of the same part-of-speech (POS).
This means that for the verb *play* I would add the frequencies of play (used as
a verb), plays (used as a verb), played, playing; but not of play/plays used as
nouns (singular and plural).
Of course I will rely on the automatic POS tagging for this, which in itself
might contain some errors.

```{r}
freq_lemma <- read_tsv(
  "frequency_measures/filtered_lemma_unigram_freqs_sv.tsv", col_names = FALSE
  ) %>%
  rename(lemma = X1, lemma_freq = X2)
head(freq_lemma)
```


Parts-of-speech (POS) tagging

```{r}
# We will trust the parts-of-speech (POS) tagged lemmatization, even if some of
# the POS assignments are probably off. We hope that they are in the right 
# ballpark.
# Divide "lemma" column into the actual lemma and the POS tag (using regular
# expressions (regex), see https://www.regular-expressions.info/rlanguage.html)
freq_lemma$lemma_true <- sub("(.*)_([a-z]*)$", "\\1", freq_lemma$lemma)
freq_lemma$POS <- sub("(.*)_([a-z]*)$", "\\2", freq_lemma$lemma)
head(freq_lemma)
# This is mostly what we want, but it also results in some inaccuracies because
# not all lemmas actually have a tag.
# To illustrate, consider some of the unique values in the POS column:
unique(freq_lemma$POS)[1:12]
# Some of them are clearly not POS tags, but the verbs themselves!
# But it's a pain to sort this out, so let's just ignore those exceptional cases
# and focus on lemmas tagged as verbs. Hopefully this will only remove some
# evenly distributed noise.
freq_verblemmas <- freq_lemma %>% filter(POS == "verb")
```


Add that info to our growing `stim` data frame:

```{r}
stim <- stim %>%
  left_join(freq_verblemmas %>%
              mutate(lemma_logfreq = log10(lemma_freq)) %>%
              select(Word = lemma_true, lemma_logfreq))
head(stim, 4) %>% kable
```



### Grammatical ambiguity (of the lemma)

We define grammatical ambiguity as 1 minus the proportion of verb occurrences
of a lemma divided by all occurrences of the same lemma (tagged as any POS).
Thus we obtain a score between 0 and 1:

- A score close to zero tells us that there is little ambiguity, i.e. the word
almost always appears as a verb
- A score close to 1 would tell us that there is maximal ambiguity, i.e. the
word hardly ever occurs as a verb.
- Note that this number only tells us about relative occurrence (proportion of
occurrences) so it doesn't take into account if a word is frequent or not.

Note that this is not giving us the grammatical ambiguity of the wordfrom itself
(e.g., "play") but of its lemma. This is a limitation imposed by the nature of
the corpus being used (see correspondence with Jeroen van Paridon, especially
emails from or around Mon 05-06, 3:00 PM, Subject: "automatically collecting
psycholinguistic features of Swedish words from corpora?")


```{r}
# Select only verb occurrences
gramm_ambig <- freq_lemma %>%
  filter(POS == "verb") %>% select(lemma_true, lemma_freq) %>%
  # join with the overall frequency
  left_join(freq_lemma %>%
              group_by(lemma_true) %>%
              summarise(overall_freq = sum(lemma_freq))
            ) %>%
  mutate(POS_ambig = 1 - lemma_freq / overall_freq)
head(gramm_ambig)
```

Add info to our growing `stim` data frame:

```{r}
stim <- stim %>%
  left_join(gramm_ambig %>%
              select(Word = lemma_true, POS_ambig))
head(stim, 4) %>% kable
```



### Bi- and trigram frequency ($log_{10}$ frequency)

These two measures per word are computed from raw bigram frequencies in the
corpus. The script that does the computation is 
`1809_norming_analysis/bi-trigram_freqs.R`.

```{r}
# Bigram frequency
bigrams <- read_csv("frequency_measures/word_bigram_freqs_sv.csv")
head(bigrams)
# Trigram frequency
trigrams <- read_csv("frequency_measures/word_trigram_freqs_sv.csv")
head(trigrams)
```


Add that info to our growing `stim` data frame:

```{r}
stim <- stim %>%
  left_join(bigrams) %>%
  left_join(trigrams)
head(stim, 4) %>% kable
```


### Number of letters

Add number of letters:

```{r}
stim$nletters <- nchar(stim$Word)
```



### Number of phonemes

Margareta M counted the phonemes. Here are some of her explanatory comments:


>
- I have made a /phonemic/, not a [phonetic] transcription, since we've decided
that we won’t be concentrating on the quality of vowels and consonants stemming
from their length (although I did mark the length with a colon). But even if we
used a phonetic transcription, it would hardly change the number of phonemes in
each word.
- I have therefore only concentrated on phonemes and not on different allophones,
and I didn’t pay any attention to the vowel- and consonant length. The quality
of vowels and consonants that I have taken into consideration is only of the
kind that is meaning differentiating.
- Thus, the phoneme /s/ corresponds to such allophones as [s] and  [ş] and to 
such orthographic symbols as \<s\>, \<c\> and \<rs\> (eg. stol, cykel, barstol).
I have counted them as one phoneme, /s/ (stol = 4 phonemes, cykel = 5 phonemes, 
barstool = 6 phonemes). That means that I have counted all the supradental sounds
as one phoneme (since /r/ disappears as it “melts in” with other consonants). 
>
> I hope I have understood the mail correspondence correctly!
>
In order to transform the (kind of) phonetic transcription offered by Lexin,
I have used 
[Tomas Riad’s document]([https://www.su.se/polopoly_fs/1.29950.1320939955!/ArtikulatoriskFonetik.pdf) (page 5-6). By means of that, the transcription is more consequent and
simplified, without claiming phonetic correctness.



```{r}
# Phoneme count
phonemes <- read_csv("swe-verbs_phoneme-count.csv")
head(phonemes) %>% kable
```


Add that info to our growing `stim` data frame:

```{r}
stim <- stim %>%
  left_join(phonemes %>% select(Word = verb, nphon = nb_phonemes))
head(stim, 4) %>% kable
```


### Number of letters

Add number of letters:

```{r}
stim$nletters <- nchar(stim$Word)
```






### Variable names to lower case

Only for consistency and stylstic purism:

```{r}
stim <- stim %>% rename(word = Word, category = Category, arm = Arm, leg = Leg,
                        pos_ambig = POS_ambig)
```



Verb selection
==============

Our constraints are as follows. We want:

- An equal amount of arm and leg verbs
(there are `r sum(stim$category == "arm")` arm verbs and 
`r sum(stim$category == "leg")` leg verbs among the 120 verbs in the list)
- The number of verbs per category has to be a multiple of 4
- The arm and leg bias in each category should be maximized
- The verbs should be matched across categories for all the other variables


A quick look at arm/leg bias
----------------------------

How well does our general arm/leg bias measure (arm-relatedness minus 
leg-relatedness rating) capture the individual arm and leg ratings?

```{r}
ggplot(stim, aes(x = arm, y = leg, colour = category)) +
  geom_point(alpha = .5) +
  geom_hline(yintercept = 4, linetype = "dashed") +
  geom_vline(xintercept = 4, linetype = "dashed") +
  xlab("arm-relatedness") +
  ylab("leg-relatedness")
```

The verbs cluster well within each category and there are no cases where they
receive a high rating from the opposite category.


Going about selecting verbs
--------------------------

We will use a bit of a trial-and-error strategy. For that it's useful to create
a function that plots the category comparison (arm vs legs) for each of the
variables and carries out t-tests for each comparison.

First, we want to go from wide to long format:

```{r}
# From current wide format
kable(head(stim))
# to long format (we also simplify somewhat, removing redundant columns)
stiml <- gather(stim %>%
                  select(- c("arm", "leg", "bias")) %>%
                  rename(arm_leg_bias = bias_absol),
                var, value, arousal : nphon)
kable(head(stiml))
kable(tail(stiml))
```


```{r}
# Choose a meaningful order for plotting the features (rather than alphabetical)
unique(stiml$var)
stiml$var <- factor(
  stiml$var, 
  levels = c("arm_leg_bias", "arousal", "imageability", "valence", "nletters",
             "nphon", "word_logfreq", "lemma_logfreq", "bigram_logfreq", 
             "trigram_logfreq", "pos_ambig"))
```




Plotting all variables
---------------------

Convenience functions for plotting and pairwise t-tests:

```{r}
# Modify the data set to plot 2 max and 2 min values per category and variable.
# The input is going to be stiml but without certain rows, so we know its
# column names (i.e., this is not a generic function).
data4plot <- function(df) {
  df$labels <- df$word
  df %>%
    ungroup() %>%
    split(list(.$var, .$category)) %>%
    map(function(d) mutate(d, myrank = rank(value, ties.method = "first"))) %>%
    map(function(d) {
      d$labels[! d$myrank %in% c(1, 2, max(d$myrank), max(d$myrank) - 1)] <- NA
      d
      }) %>%
    bind_rows()
}

# comparison plot
compar_plot <- function(df) {
  p <- ggplot(df, aes(x = category, y = value, colour = category, label = labels)) +
    geom_boxplot() +
    geom_text_repel() +
    facet_wrap(~ var, scales = "free_y") +
    theme_bw()
  print(p)
}

# t-test for each variable. I use the nesting approach from purrr, see
# https://www.rstudio.com/resources/cheatsheets/#purrr
compar_ttest <- function(df) {
  df %>%
    ungroup() %>%
    group_by(var) %>%
    nest() %>%
    mutate(ttest = map(data, function(df) t.test(value ~ category, data = df,
                                                 var.equal = TRUE)),
           estim_arm = map_dbl(ttest, function(x) x$estimate[1]),
           estim_leg = map_dbl(ttest, function(x) x$estimate[2]),
           t  = map_dbl(ttest, "statistic"),
           df = map_dbl(ttest, "parameter"),
           p  = map_dbl(ttest, "p.value")) %>%
    select(-data, -ttest)
}

# wrapper function
compar_wrap <- function (df) {
  df %>% data4plot %>% compar_plot
  compar_ttest(df)
}
```


Comparison of the full list:

```{r, fig.height=7, fig.width=10}
compar_wrap(stiml)
```



Trial and error
--------------

We need to reduce the number of arm and leg verbs to 52 in each category
(from `r sum(stim$category == "arm")` and `r sum(stim$category == "leg")`, 
respectively).

```{r}
# Create two dataframes with our current stimuli selection - from them we are
# going to remove some verbs and plot the results + run t-tests iteratively:
sel_l <- stiml  # long format needed for plotting
sel_w <- stim   # wide format needed for looking up extreme values more easily
```


A handy function is one that gives you the `n` verbs in a category (`categ` =
"arm"/"leg") that have the lowest/highest (`tail` = "min"/"max") value on some
variable (`var`):

```{r}
extreme_val <- function(categ, var, tail, n = 2, df = sel_w) {
  df <- df %>% filter(category == categ)
  ordered <- df$word[order(df[[var]])]
  l <- list(min = ordered[1 : n],
            max = ordered[(length(ordered) - n + 1) : length(ordered)])
  l[[tail]]
}
# E.g.:
extreme_val("arm", "valence", "min")
extreme_val("leg", "valence", "max")
```


```{r}
# Function to remove words from both sel_l and sel_w
remove_word <- function (categ, var, tail, n = 2, df = sel_w) {
  words2remove <- extreme_val(categ, var, tail, n, df)
  cat(paste("Removing the", n, tail, categ, "words for", var, ":\n"))
  cat(words2remove)
  # The <<- assignment modifies objects in the workspace (bypassing usual scope)
  sel_l <<- sel_l %>% filter(! word %in% words2remove)
  sel_w <<- sel_w %>% filter(! word %in% words2remove)
}
```



We start by removing low frequency words (specifically low *lemma* frequency):

```{r}
stim[order(stim$lemma_logfreq), ][1:8, ] %>% kable
```


```{r}
remove_word("arm", "lemma_logfreq", "min", 5)
remove_word("leg", "lemma_logfreq", "min", 1)
```


```{r, fig.height=7, fig.width=10}
compar_wrap(sel_l)
```


To fix imageability, remove least imageable leg word and some highly imageable
arm words:
  
```{r}
remove_word("arm", "imageability", "max", 3)
remove_word("leg", "imageability", "min", 1)
```

```{r, fig.height=7, fig.width=10}
compar_wrap(sel_l)
```


This is our current count of verbs:
  
```{r}
with(unique(sel_l[, c("word", "category")]), table(category))
```


Fix word frequency:
  
```{r}
remove_word("arm", "word_logfreq", "max", 1)
```

```{r, fig.height=7, fig.width=10}
compar_wrap(sel_l)
```


This is our current count of verbs:
  
```{r}
with(unique(sel_l[, c("word", "category")]), table(category))
```



We need to remove 4 arm verbs and 1 leg verb and main difference continues to be
imageability:
  
  
```{r}
remove_word("arm", "imageability", "max", 4)
remove_word("leg", "imageability", "min", 1)
```

```{r, fig.height=7, fig.width=10}
compar_wrap(sel_l)
```



  
```{r}
with(unique(sel_l[, c("word", "category")]), table(category))
```


Final list
==========

Here is the final list of verbs:

```{r}
sel_w <- sel_w %>% arrange(category, word)
sel_w %>% kable
```

```{r}
# save to disk:
# verbs
sel_w %>% write_csv("sw_verb_selection_1906.csv")
sel_w %>% write_csv("../1908_sp13_replic_swe/psychopy_exp/stimuli/sw_verb_selection_1906.csv")
# t-test comparison of arm/leg verbs along each variable
compar_ttest(sel_l) %>% write_csv("sw_verb_selection_t-test_1906.csv")
```


```{r}
# Mean and SE for each category and variable (as in SP13 Table 1)
sel_l %>% ungroup() %>% group_by(var, category) %>%
  summarise(M  = mean(value, na.rm = TRUE),
            SD = sd(value, na.rm = TRUE),
            SE = SD / sqrt(n())) %>%
  write_csv("sw_verb_selection_means_SE_1906.csv")
```

